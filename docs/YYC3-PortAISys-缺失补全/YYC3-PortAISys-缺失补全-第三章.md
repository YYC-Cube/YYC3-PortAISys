
# YYCÂ³-AILP æ™ºèƒ½æµ®çª—ç³»ç»Ÿ - ç¼ºå¤±è¡¥å…¨å®Œæ•´æ–¹æ¡ˆï¼ˆç¬¬ä¸‰ç« ï¼‰

â”‚ ç»§ç»­æˆ‘ä»¬çš„æ—…ç¨‹ï¼ ğŸŒŸ å‰ä¸¤ç« æˆ‘ä»¬å®Œæˆäº†åŸºç¡€è®¾æ–½å’Œæ ¸å¿ƒå¼•æ“ï¼Œä»Šå¤©æˆ‘ä»¬è¿›å…¥ç³»ç»Ÿçš„"å¤§è„‘"â€”â€”æ¨¡å‹é€‚é…å™¨ã€‚
â”‚ æ¨¡å‹é€‚é…å™¨æ˜¯è¿æ¥AIæ¨¡å‹å’Œæˆ‘ä»¬ç³»ç»Ÿçš„æ¡¥æ¢ï¼Œè®¾è®¡å¾—å½“å¯ä»¥è®©æˆ‘ä»¬è½»æ¾åˆ‡æ¢å’Œç»„åˆä¸åŒçš„AIèƒ½åŠ›ã€‚
â”‚ è®©æˆ‘ä»¬ä¸€èµ·æ„å»ºä¸€ä¸ªå¼ºå¤§ã€çµæ´»ã€å¯æ‰©å±•çš„æ¨¡å‹é€‚é…ç³»ç»Ÿï¼ğŸ’ª

--------

## ğŸ“‹ ç¬¬ä¸‰ç« ï¼šæ¨¡å‹é€‚é…å™¨åŸºç¡€å®ç°

### 3.1 å¯¼å¸ˆå¯„è¯­ ğŸŒŸ

â”‚ "ä¸è¦æŠŠAIæ¨¡å‹å½“ä½œé»‘ç›’ï¼Œè€Œæ˜¯è¦æŠŠå®ƒä»¬å½“ä½œå¯æ’æ‹”çš„ç»„ä»¶ã€‚" æ¨¡å‹é€‚é…å™¨çš„æ ¸å¿ƒä»·å€¼åœ¨äºï¼šæŠ½è±¡ä¸æ ‡å‡†åŒ–ã€‚
â”‚ é€šè¿‡ç»Ÿä¸€çš„æ¥å£ï¼Œæˆ‘ä»¬å¯ä»¥æ— ç¼åˆ‡æ¢OpenAIã€Anthropicã€æœ¬åœ°æ¨¡å‹ç­‰ä¸åŒæä¾›å•†ï¼Œ
â”‚ ç”šè‡³å¯ä»¥åœ¨ä¸€æ¬¡å¯¹è¯ä¸­åŠ¨æ€é€‰æ‹©æœ€é€‚åˆçš„æ¨¡å‹ã€‚è¿™å°±æ˜¯YYCÂ³ç³»ç»Ÿæ™ºèƒ½åŒ–çš„åŸºç¡€ï¼

### 3.2 æ¨¡å‹é€‚é…å™¨æ ¸å¿ƒç±»å‹å®šä¹‰

  // ğŸ“¦ src/types/model.types.ts - æ¨¡å‹é€‚é…å™¨ç±»å‹å®šä¹‰

  /**

   * ================== åŸºç¡€ç±»å‹å®šä¹‰ ==================
   */

  /**

   * æ¨¡å‹æä¾›å•†æšä¸¾
   */
  export enum ModelProvider {
    OPENAI = 'openai',
    ANTHROPIC = 'anthropic',
    GOOGLE = 'google',
    AZURE = 'azure',
    LOCAL = 'local',
    CUSTOM = 'custom'
  }

  /**

   * æ¨¡å‹ç±»å‹æšä¸¾
   */
  export enum ModelType {
    // å¤§è¯­è¨€æ¨¡å‹
    LLM = 'llm',
    CHAT_LLM = 'chat_llm',

    // å¤šæ¨¡æ€æ¨¡å‹
    MULTIMODAL = 'multimodal',
    VISION = 'vision',
    AUDIO = 'audio',

    // ä¸“ç”¨æ¨¡å‹
    EMBEDDING = 'embedding',
    CODE = 'code',
    TRANSLATION = 'translation',
    CLASSIFICATION = 'classification'
  }

  /**

   * æ¨¡å‹èƒ½åŠ›æšä¸¾
   */
  export enum ModelCapability {
    TEXT_GENERATION = 'text_generation',
    TEXT_COMPLETION = 'text_completion',
    CHAT_COMPLETION = 'chat_completion',
    STREAMING = 'streaming',
    FUNCTION_CALLING = 'function_calling',
    IMAGE_GENERATION = 'image_generation',
    VISION_UNDERSTANDING = 'vision_understanding',
    AUDIO_TRANSCRIPTION = 'audio_transcription',
    EMBEDDING = 'embedding'
  }

  /**

   * ================== è¯·æ±‚ç±»å‹å®šä¹‰ ==================
   */

  /**

   * åŸºç¡€è¯·æ±‚æ¥å£
   */
  export interface BaseRequest {
    provider: ModelProvider;
    model: string;
    timeout?: number;
    metadata?: Record<string, unknown>;
  }

  /**

   * æ–‡æœ¬è¡¥å…¨è¯·æ±‚
   */
  export interface CompletionRequest extends BaseRequest {
    type: ModelType.LLM;
    prompt: string;
    parameters?: {
      maxTokens?: number;
      temperature?: number;
      topP?: number;
      topK?: number;
      frequencyPenalty?: number;
      presencePenalty?: number;
      stopSequences?: string[];
      logitBias?: Record<number, number>;
    };
  }

  /**

   * èŠå¤©è¡¥å…¨è¯·æ±‚
   */
  export interface ChatRequest extends BaseRequest {
    type: ModelType.CHAT_LLM;
    messages: ChatMessage[];
    parameters?: {
      maxTokens?: number;
      temperature?: number;
      topP?: number;
      frequencyPenalty?: number;
      presencePenalty?: number;
      stopSequences?: string[];
      functionCall?: 'auto' | 'none' | { name: string };
      functions?: FunctionDefinition[];
    };
  }

  /**

   * èŠå¤©æ¶ˆæ¯æ¥å£
   */
  export interface ChatMessage {
    role: 'system' | 'user' | 'assistant' | 'function';
    content: string | MultimodalContent[];
    name?: string;
    functionCall?: {
      name: string;
      arguments: string;
    };
  }

  /**

   * å¤šæ¨¡æ€å†…å®¹
   */
  export interface MultimodalContent {
    type: 'text' | 'image_url' | 'audio';
    text?: string;
    imageUrl?: {
      url: string;
      detail?: 'auto' | 'low' | 'high';
    };
  }

  /**

   * å‡½æ•°å®šä¹‰æ¥å£
   */
  export interface FunctionDefinition {
    name: string;
    description: string;
    parameters: {
      type: string;
      properties?: Record<string, unknown>;
      required?: string[];
    };
  }

  /**

   * å‘é‡åµŒå…¥è¯·æ±‚
   */
  export interface EmbeddingRequest extends BaseRequest {
    type: ModelType.EMBEDDING;
    input: string | string[];
    parameters?: {
      dimensions?: number;
      model?: string;
    };
  }

  /**

   * æµå¼è¯·æ±‚
   */
  export interface StreamRequest extends BaseRequest {
    stream: true;
    streamOptions?: {
      includeUsage?: boolean;
    };
  }

  /**

   * ================== å“åº”ç±»å‹å®šä¹‰ ==================
   */

  /**

   * åŸºç¡€å“åº”æ¥å£
   */
  export interface BaseResponse {
    success: boolean;
    model: string;
    provider: ModelProvider;
    timestamp: Date;
    processingTime: number;
    traceId?: string;
    error?: ErrorResponse;
  }

  /**

   * æ–‡æœ¬è¡¥å…¨å“åº”
   */
  export interface CompletionResponse extends BaseResponse {
    type: ModelType.LLM;
    content: string;
    finishReason: 'stop' | 'length' | 'content_filter' | 'unknown';
    usage: {
      promptTokens: number;
      completionTokens: number;
      totalTokens: number;
    };
    alternatives?: string[];
  }

  /**

   * èŠå¤©è¡¥å…¨å“åº”
   */
  export interface ChatResponse extends BaseResponse {
    type: ModelType.CHAT_LLM;
    message: ChatMessage;
    finishReason: 'stop' | 'length' | 'function_call' | 'content_filter' | 'unknown';
    usage: {
      promptTokens: number;
      completionTokens: number;
      totalTokens: number;
    };
    functionCall?: {
      name: string;
      arguments: string;
    };
  }

  /**

   * å‘é‡åµŒå…¥å“åº”
   */
  export interface EmbeddingResponse extends BaseResponse {
    type: ModelType.EMBEDDING;
    embeddings: number[] | number[][];
    dimensions: number;
    usage: {
      promptTokens: number;
      totalTokens: number;
    };
  }

  /**

   * é”™è¯¯å“åº”
   */
  export interface ErrorResponse {
    code: string;
    message: string;
    details?: Record<string, unknown>;
    retryable?: boolean;
    suggestedRetryAfter?: number; // ç§’
  }

  /**

   * ================== æµå¼ç±»å‹å®šä¹‰ ==================
   */

  /**

   * æµå¼æ•°æ®å—
   */
  export interface StreamChunk {
    delta: {
      content?: string;
      role?: string;
      functionCall?: {
        name?: string;
        arguments?: string;
      };
    };
    finishReason?: string;
    index: number;
    isComplete: boolean;
    usage?: {
      promptTokens: number;
      completionTokens: number;
      totalTokens: number;
    };
  }

  /**

   * ================== æ¨¡å‹ä¿¡æ¯ç±»å‹ ==================
   */

  /**

   * æ¨¡å‹ä¿¡æ¯æ¥å£
   */
  export interface ModelInfo {
    id: string;
    name: string;
    provider: ModelProvider;
    type: ModelType;
    capabilities: ModelCapability[];
    pricing?: {
      inputPrice: number;  // æ¯1K tokensä»·æ ¼
      outputPrice: number;  // æ¯1K tokensä»·æ ¼
      currency: string;
    };
    limits?: {
      maxInputTokens: number;
      maxOutputTokens: number;
      maxRequestsPerMinute: number;
      maxTokensPerMinute: number;
    };
    version?: string;
    status: 'available' | 'deprecated' | 'experimental';
  }

  /**

   * ================== é€‚é…å™¨é…ç½®ç±»å‹ ==================
   */

  /**

   * æ¨¡å‹é€‚é…å™¨é…ç½®
   */
  export interface ModelAdapterConfig {
    provider: ModelProvider;
    apiKey?: string;
    baseURL?: string;
    model: string;
    timeout?: number;
    retryPolicy?: {
      maxRetries: number;
      backoffFactor: number;
      initialDelay: number;
    };
    cache?: {
      enabled: boolean;
      ttl: number;
      maxSize: number;
    };
    metrics?: {
      enabled: boolean;
    };
    logging?: {
      enabled: boolean;
      level: 'debug' | 'info' | 'warn' | 'error';
    };
  }

  /**

   * ================== è·¯ç”±ç›¸å…³ç±»å‹ ==================
   */

  /**

   * è·¯ç”±è¯·æ±‚
   */
  export interface RoutingRequest {
    request: CompletionRequest | ChatRequest | EmbeddingRequest;
    constraints?: {
      maxLatency?: number;        // æœ€å¤§å»¶è¿Ÿï¼ˆæ¯«ç§’ï¼‰
      maxCost?: number;           // æœ€å¤§æˆæœ¬
      minQuality?: number;        // æœ€å°è´¨é‡ï¼ˆ0-1ï¼‰
      capabilities?: ModelCapability[];  // å¿…éœ€èƒ½åŠ›
    };
    preferences?: {
      provider?: ModelProvider;
      model?: string;
    };
  }

  /**

   * è·¯ç”±å†³ç­–
   */
  export interface RoutingDecision {
    adapter: IModelAdapter;
    model: string;
    reason: string;
    score: number;
    estimatedLatency: number;
    estimatedCost: number;
  }

  /**

   * ================== æ€§èƒ½ç›‘æ§ç±»å‹ ==================
   */

  /**

   * æ¨¡å‹æ€§èƒ½æŒ‡æ ‡
   */
  export interface ModelMetrics {
    requestCount: number;
    successCount: number;
    errorCount: number;
    averageLatency: number;
    p95Latency: number;
    p99Latency: number;
    totalTokens: number;
    totalCost: number;
    errorRate: number;
  }

  /**

   * ================== å·¥å…·å‡½æ•° ==================
   */

  /**

   * ç”Ÿæˆè¿½è¸ªID
   */
  export function generateTraceId(): string {
    return `trace-${Date.now()}-${Math.random().toString(36).substring(2, 15)}`;
  }

  /**

   * è®¡ç®—æˆæœ¬
   */
  export function calculateCost(
    inputTokens: number,
    outputTokens: number,
    inputPrice: number,
    outputPrice: number
  ): number {
    return (inputTokens / 1000) * inputPrice + (outputTokens / 1000) * outputPrice;
  }

### 3.3 æ¨¡å‹é€‚é…å™¨æŠ½è±¡åŸºç±»

  // ğŸ“¦ src/model/BaseModelAdapter.ts - æ¨¡å‹é€‚é…å™¨æŠ½è±¡åŸºç±»
  import { logger } from '../utils/logger';
  import { metrics } from '../utils/metrics';
  import {
    IModelAdapter,
    ModelAdapterConfig,
    CompletionRequest,
    CompletionResponse,
    ChatRequest,
    ChatResponse,
    EmbeddingRequest,
    EmbeddingResponse,
    ModelInfo,
    ModelMetrics,
    ErrorResponse,
    generateTraceId,
    StreamChunk
  } from '../types/model.types';

  /**

   * é¢„å¤„ç†åçš„è¯·æ±‚
   */
  interface PreprocessedRequest {
    original: CompletionRequest | ChatRequest | EmbeddingRequest;
    normalized: unknown;
    headers: Record<string, string>;
    timeout: number;
    traceId: string;
  }

  /**

   * åŸå§‹æ¨¡å‹å“åº”
   */
  interface RawModelResponse {
    raw: unknown;
    normalized: unknown;
  }

  /**

   * æ¨¡å‹é€‚é…å™¨æŠ½è±¡åŸºç±»
   *
   * è®¾è®¡ç†å¿µï¼š
   * 1. æ¨¡æ¿æ–¹æ³•æ¨¡å¼ï¼šå®šä¹‰ç®—æ³•éª¨æ¶ï¼Œå­ç±»å®ç°å…·ä½“æ­¥éª¤
   * 2. ç­–ç•¥æ¨¡å¼ï¼šä¸åŒæ¨¡å‹æœ‰ä¸åŒçš„è°ƒç”¨ç­–ç•¥
   * 3. è£…é¥°å™¨æ¨¡å¼ï¼šæ·»åŠ ç¼“å­˜ã€é‡è¯•ã€ç›‘æ§ç­‰æ¨ªåˆ‡å…³æ³¨ç‚¹
   * 4. ç»Ÿä¸€æ¥å£ï¼šæ‰€æœ‰é€‚é…å™¨å®ç°ç›¸åŒçš„æ¥å£
   * 5. å¯æ‰©å±•æ€§ï¼šæ˜“äºæ·»åŠ æ–°çš„æ¨¡å‹æä¾›å•†
   */
  export abstract class BaseModelAdapter implements IModelAdapter {
    // ============ é…ç½®ä¸çŠ¶æ€ ============
    protected config: ModelAdapterConfig;
    protected modelInfo: ModelInfo;
    protected metrics: ModelMetrics;
    protected isInitialized: boolean = false;

    // ============ æ€§èƒ½ç»Ÿè®¡ ============
    private latencyHistory: number[] = [];

    constructor(config: ModelAdapterConfig, modelInfo: ModelInfo) {
      this.config = config;
      this.modelInfo = modelInfo;
      this.metrics = {
        requestCount: 0,
        successCount: 0,
        errorCount: 0,
        averageLatency: 0,
        p95Latency: 0,
        p99Latency: 0,
        totalTokens: 0,
        totalCost: 0,
        errorRate: 0
      };

      logger.info('æ¨¡å‹é€‚é…å™¨åˆ›å»º', 'BaseModelAdapter', {
        provider: config.provider,
        model: config.model
      });
    }

    // ================= æ¨¡å‹ç®¡ç† =================

    getModelInfo(): ModelInfo {
      return this.modelInfo;
    }

    async isAvailable(): Promise<boolean> {
      try {
        // ç®€å•çš„å¥åº·æ£€æŸ¥
        await this.healthCheck();
        return true;
      } catch (error) {
        logger.warn('æ¨¡å‹ä¸å¯ç”¨', 'BaseModelAdapter', {
          provider: this.config.provider,
          model: this.config.model,
          error
        });
        return false;
      }
    }

    async healthCheck(): Promise<{ status: string; latency?: number; message?: string }> {
      const startTime = Date.now();

      try {
        // å­ç±»å®ç°å…·ä½“çš„å¥åº·æ£€æŸ¥é€»è¾‘
        await this.performHealthCheck();

        const latency = Date.now() - startTime;

        return {
          status: 'healthy',
          latency,
          message: `${this.config.provider} ${this.config.model} is healthy`
        };
      } catch (error) {
        const latency = Date.now() - startTime;

        return {
          status: 'unhealthy',
          latency,
          message: error instanceof Error ? error.message : 'Unknown error'
        };
      }
    }

    // ================= æ ¸å¿ƒæ¨ç†æ–¹æ³• =================

    /**
     * æ–‡æœ¬è¡¥å…¨ï¼ˆæ¨¡æ¿æ–¹æ³•ï¼‰
     */
    async generateCompletion(request: CompletionRequest): Promise<CompletionResponse> {
      const startTime = Date.now();
      const traceId = generateTraceId();

      logger.debug('å¼€å§‹æ–‡æœ¬è¡¥å…¨', 'BaseModelAdapter', {
        model: this.config.model,
        prompt: request.prompt.substring(0, 100) + '...',
        traceId
      });

      try {
        // 1. æ›´æ–°æŒ‡æ ‡
        this.metrics.requestCount++;

        // 2. é¢„å¤„ç†è¯·æ±‚
        const preprocessed = await this.preprocessRequest(request, traceId);

        // 3. æ£€æŸ¥ç¼“å­˜
        const cached = await this.checkCache(preprocessed);
        if (cached) {
          logger.debug('å‘½ä¸­ç¼“å­˜', 'BaseModelAdapter', { traceId });
          metrics.increment('model_adapter.cache_hit');
          return cached;
        }

        // 4. è°ƒç”¨æ¨¡å‹APIï¼ˆæŠ½è±¡æ–¹æ³•ï¼Œç”±å­ç±»å®ç°ï¼‰
        const rawResponse = await this.callModelAPI(preprocessed);

        // 5. åå¤„ç†å“åº”
        const processed = await this.postprocessResponse(rawResponse, request);

        // 6. ç¼“å­˜ç»“æœ
        await this.cacheResult(preprocessed, processed);

        // 7. æ›´æ–°æ€§èƒ½æŒ‡æ ‡
        const processingTime = Date.now() - startTime;
        this.updateMetrics(processingTime, processed.usage.totalTokens, true);

        // 8. è®°å½•ç›‘æ§æŒ‡æ ‡
        metrics.increment('model_adapter.completion_success');
        metrics.histogram('model_adapter.completion_latency', processingTime);
        metrics.histogram('model_adapter.tokens_used', processed.usage.totalTokens);

        logger.debug('æ–‡æœ¬è¡¥å…¨å®Œæˆ', 'BaseModelAdapter', {
          processingTime,
          tokens: processed.usage.totalTokens,
          traceId
        });

        return processed;
      } catch (error) {
        // é”™è¯¯å¤„ç†
        const processingTime = Date.now() - startTime;
        this.updateMetrics(processingTime, 0, false);

        logger.error('æ–‡æœ¬è¡¥å…¨å¤±è´¥', 'BaseModelAdapter', {
          error,
          processingTime,
          traceId
        });

        metrics.increment('model_adapter.completion_failed');

        // å°è¯•é”™è¯¯æ¢å¤
        return await this.handleError(error, request, traceId);
      }
    }

    /**
     * èŠå¤©è¡¥å…¨ï¼ˆæ¨¡æ¿æ–¹æ³•ï¼‰
     */
    async generateChatCompletion(request: ChatRequest): Promise<ChatResponse> {
      const startTime = Date.now();
      const traceId = generateTraceId();

      logger.debug('å¼€å§‹èŠå¤©è¡¥å…¨', 'BaseModelAdapter', {
        model: this.config.model,
        messageCount: request.messages.length,
        traceId
      });

      try {
        // 1. æ›´æ–°æŒ‡æ ‡
        this.metrics.requestCount++;

        // 2. é¢„å¤„ç†è¯·æ±‚
        const preprocessed = await this.preprocessRequest(request, traceId);

        // 3. æ£€æŸ¥ç¼“å­˜
        const cached = await this.checkCache(preprocessed);
        if (cached) {
          logger.debug('å‘½ä¸­ç¼“å­˜', 'BaseModelAdapter', { traceId });
          metrics.increment('model_adapter.cache_hit');
          return cached;
        }

        // 4. è°ƒç”¨æ¨¡å‹API
        const rawResponse = await this.callModelAPI(preprocessed);

        // 5. åå¤„ç†å“åº”
        const processed = await this.postprocessResponse(rawResponse, request);

        // 6. ç¼“å­˜ç»“æœ
        await this.cacheResult(preprocessed, processed);

        // 7. æ›´æ–°æ€§èƒ½æŒ‡æ ‡
        const processingTime = Date.now() - startTime;
        this.updateMetrics(processingTime, processed.usage.totalTokens, true);

        // 8. è®°å½•ç›‘æ§æŒ‡æ ‡
        metrics.increment('model_adapter.chat_success');
        metrics.histogram('model_adapter.chat_latency', processingTime);
        metrics.histogram('model_adapter.tokens_used', processed.usage.totalTokens);

        logger.debug('èŠå¤©è¡¥å…¨å®Œæˆ', 'BaseModelAdapter', {
          processingTime,
          tokens: processed.usage.totalTokens,
          traceId
        });

        return processed;
      } catch (error) {
        // é”™è¯¯å¤„ç†
        const processingTime = Date.now() - startTime;
        this.updateMetrics(processingTime, 0, false);

        logger.error('èŠå¤©è¡¥å…¨å¤±è´¥', 'BaseModelAdapter', {
          error,
          processingTime,
          traceId
        });

        metrics.increment('model_adapter.chat_failed');

        return await this.handleError(error, request, traceId);
      }
    }

    /**
     * ç”Ÿæˆå‘é‡åµŒå…¥
     */
    async generateEmbedding(request: EmbeddingRequest): Promise<EmbeddingResponse> {
      const startTime = Date.now();
      const traceId = generateTraceId();

      logger.debug('å¼€å§‹ç”Ÿæˆå‘é‡åµŒå…¥', 'BaseModelAdapter', {
        model: this.config.model,
        input: Array.isArray(request.input) ? `${request.input.length} items` : request.input.substring(0, 100),
        traceId
      });

      try {
        this.metrics.requestCount++;

        const preprocessed = await this.preprocessRequest(request, traceId);
        const cached = await this.checkCache(preprocessed);

        if (cached) {
          return cached;
        }

        const rawResponse = await this.callModelAPI(preprocessed);
        const processed = await this.postprocessResponse(rawResponse, request);
        await this.cacheResult(preprocessed, processed);

        const processingTime = Date.now() - startTime;
        this.updateMetrics(processingTime, processed.usage.totalTokens, true);

        return processed;
      } catch (error) {
        const processingTime = Date.now() - startTime;
        this.updateMetrics(processingTime, 0, false);

        logger.error('ç”Ÿæˆå‘é‡åµŒå…¥å¤±è´¥', 'BaseModelAdapter', { error, processingTime });

        throw error;
      }
    }

    // ================= æµå¼å¤„ç†æ–¹æ³• =================

    /**
     * æµå¼æ–‡æœ¬è¡¥å…¨
     */
    async *streamCompletion(request: CompletionRequest): AsyncIterable<StreamChunk> {
      const traceId = generateTraceId();
      let buffer = '';
      let tokenCount = 0;

      logger.debug('å¼€å§‹æµå¼æ–‡æœ¬è¡¥å…¨', 'BaseModelAdapter', {
        model: this.config.model,
        traceId
      });

      try {
        this.metrics.requestCount++;

        const preprocessed = await this.preprocessRequest(request, traceId);
        const stream = await this.callModelStream(preprocessed);

        for await (const chunk of stream) {
          const parsed = this.parseStreamChunk(chunk);
          tokenCount += parsed.tokens || 0;
          buffer += parsed.text || '';

          const streamChunk: StreamChunk = {
            delta: {
              content: parsed.text
            },
            finishReason: parsed.finishedReason,
            index: parsed.index || 0,
            isComplete: parsed.finished || false
          };

          yield streamChunk;

          if (parsed.finished) {
            // å®Œæˆæ—¶çš„æœ€ç»ˆchunk
            yield {
              delta: {},
              finishReason: parsed.finishedReason,
              index: parsed.index || 0,
              isComplete: true,
              usage: {
                promptTokens: tokenCount, // ç®€åŒ–å¤„ç†
                completionTokens: tokenCount,
                totalTokens: tokenCount * 2
              }
            };
            break;
          }
        }
      } catch (error) {
        logger.error('æµå¼æ–‡æœ¬è¡¥å…¨å¤±è´¥', 'BaseModelAdapter', { error, traceId });
        throw error;
      }
    }

    // ================= æ‰¹é‡å¤„ç†æ–¹æ³• =================

    /**
     * æ‰¹é‡æ–‡æœ¬è¡¥å…¨
     */
    async batchComplete(requests: CompletionRequest[]): Promise<CompletionResponse[]> {
      logger.debug('å¼€å§‹æ‰¹é‡æ–‡æœ¬è¡¥å…¨', 'BaseModelAdapter', {
        count: requests.length
      });

      const results = await Promise.allSettled(
        requests.map(request => this.generateCompletion(request))
      );

      return results.map((result, index) => {
        if (result.status === 'fulfilled') {
          return result.value;
        } else {
          // è¿”å›é”™è¯¯å“åº”
          return {
            success: false,
            model: this.config.model,
            provider: this.config.provider,
            timestamp: new Date(),
            processingTime: 0,
            content: '',
            finishReason: 'unknown',
            usage: {
              promptTokens: 0,
              completionTokens: 0,
              totalTokens: 0
            },
            error: {
              code: 'BATCH_ERROR',
              message: result.reason instanceof Error ? result.reason.message : 'Unknown error'
            }
          };
        }
      });
    }

    // ================= æ¨¡å‹é…ç½®æ–¹æ³• =================

    async updateConfig(config: Partial<ModelAdapterConfig>): Promise<void> {
      logger.info('æ›´æ–°æ¨¡å‹é€‚é…å™¨é…ç½®', 'BaseModelAdapter', {
        provider: this.config.provider,
        model: this.config.model,
        updates: Object.keys(config)
      });

      this.config = { ...this.config, ...config };

      // å¦‚æœéœ€è¦é‡æ–°åˆå§‹åŒ–
      if (!this.isInitialized) {
        await this.initialize();
      }
    }

    getConfig(): ModelAdapterConfig {
      return { ...this.config };
    }

    // ================= æ€§èƒ½ä¼˜åŒ–æ–¹æ³• =================

    async warmup(): Promise<void> {
      logger.info('æ¨¡å‹é¢„çƒ­ä¸­...', 'BaseModelAdapter', {
        provider: this.config.provider,
        model: this.config.model
      });

      try {
        // å‘é€ä¸€ä¸ªç®€å•çš„è¯·æ±‚è¿›è¡Œé¢„çƒ­
        await this.generateCompletion({
          provider: this.config.provider,
          model: this.config.model,
          type: 'llm' as any,
          prompt: 'Hello'
        });

        logger.info('æ¨¡å‹é¢„çƒ­å®Œæˆ', 'BaseModelAdapter');
      } catch (error) {
        logger.warn('æ¨¡å‹é¢„çƒ­å¤±è´¥', 'BaseModelAdapter', { error });
        // ä¸æŠ›å‡ºé”™è¯¯ï¼Œé¢„çƒ­å¤±è´¥ä¸åº”å½±å“åç»­ä½¿ç”¨
      }
    }

    async clearCache(): Promise<void> {
      // ç”±å­ç±»å®ç°
      logger.info('æ¸…é™¤ç¼“å­˜', 'BaseModelAdapter');
    }

    async optimizeFor(batchSize: number): Promise<void> {
      logger.info('ä¼˜åŒ–æ¨¡å‹é€‚é…å™¨', 'BaseModelAdapter', { batchSize });
      // ç”±å­ç±»å®ç°ç‰¹å®šä¼˜åŒ–
    }

    // ================= æ€§èƒ½æŒ‡æ ‡æ–¹æ³• =================

    getMetrics(): ModelMetrics {
      return { ...this.metrics };
    }

    // ================= æŠ½è±¡æ–¹æ³•ï¼ˆç”±å­ç±»å®ç°ï¼‰=================

    /**
     * åˆå§‹åŒ–é€‚é…å™¨
     */
    abstract initialize(): Promise<void>;

    /**
     * è°ƒç”¨æ¨¡å‹API
     */
    protected abstract callModelAPI(request: PreprocessedRequest): Promise<RawModelResponse>;

    /**
     * è°ƒç”¨æ¨¡å‹æµå¼API
     */
    protected abstract callModelStream(request: PreprocessedRequest): AsyncIterable<unknown>;

    /**
     * è§£ææµå¼æ•°æ®å—
     */
    protected abstract parseStreamChunk(chunk: unknown): {
      text?: string;
      tokens?: number;
      finished?: boolean;
      finishedReason?: string;
      index?: number;
    };

    /**
     * æ‰§è¡Œå¥åº·æ£€æŸ¥
     */
    protected abstract performHealthCheck(): Promise<void>;

    // ================= ä¿æŠ¤æ–¹æ³•ï¼ˆä¾›å­ç±»ä½¿ç”¨ï¼‰=================

    /**
     * é¢„å¤„ç†è¯·æ±‚
     */
    protected async preprocessRequest(
      request: CompletionRequest | ChatRequest | EmbeddingRequest,
      traceId: string
    ): Promise<PreprocessedRequest> {
      // 1. éªŒè¯è¯·æ±‚
      await this.validateRequest(request);

      // 2. æ ‡å‡†åŒ–è¯·æ±‚
      const normalized = this.normalizeRequest(request);

      // 3. æ„å»ºè¯·æ±‚å¤´
      const headers = this.buildHeaders();

      return {
        original: request,
        normalized,
        headers,
        timeout: this.config.timeout || 30000,
        traceId
      };
    }

    /**
     * åå¤„ç†å“åº”
     */
    protected async postprocessResponse(
      rawResponse: RawModelResponse,
      originalRequest: CompletionRequest | ChatRequest | EmbeddingRequest
    ): Promise<CompletionResponse | ChatResponse | EmbeddingResponse> {
      // å­ç±»å¯ä»¥è¦†ç›–æ­¤æ–¹æ³•å®ç°ç‰¹å®šçš„åå¤„ç†é€»è¾‘
      return rawResponse.normalized as any;
    }

    /**
     * æ£€æŸ¥ç¼“å­˜
     */
    protected async checkCache(request: PreprocessedRequest): Promise<CompletionResponse | ChatResponse |
EmbeddingResponse | null> {
      if (!this.config.cache?.enabled) {
        return null;
      }

      const cacheKey = this.generateCacheKey(request);
      logger.debug('æ£€æŸ¥ç¼“å­˜', 'BaseModelAdapter', { cacheKey });

      // ç”±å­ç±»å®ç°å…·ä½“çš„ç¼“å­˜é€»è¾‘
      // è¿™é‡Œè¿”å›nullè¡¨ç¤ºæœªå‘½ä¸­
      return null;
    }

    /**
     * ç¼“å­˜ç»“æœ
     */
    protected async cacheResult(
      request: PreprocessedRequest,
      response: CompletionResponse | ChatResponse | EmbeddingResponse
    ): Promise<void> {
      if (!this.config.cache?.enabled) {
        return;
      }

      const cacheKey = this.generateCacheKey(request);
      logger.debug('ç¼“å­˜ç»“æœ', 'BaseModelAdapter', { cacheKey });

      // ç”±å­ç±»å®ç°å…·ä½“çš„ç¼“å­˜é€»è¾‘
    }

    /**
     * é”™è¯¯å¤„ç†
     */
    protected async handleError(
      error: unknown,
      originalRequest: CompletionRequest | ChatRequest | EmbeddingRequest,
      traceId: string
    ): Promise<CompletionResponse | ChatResponse | EmbeddingResponse> {
      const errorResponse: ErrorResponse = {
        code: 'MODEL_ADAPTER_ERROR',
        message: error instanceof Error ? error.message : 'Unknown error',
        retryable: false
      };

      // å°è¯•é”™è¯¯åˆ†ç±»
      if (error instanceof Error) {
        if (error.message.includes('timeout') || error.message.includes('ETIMEDOUT')) {
          errorResponse.code = 'TIMEOUT_ERROR';
          errorResponse.retryable = true;
          errorResponse.suggestedRetryAfter = 5;
        } else if (error.message.includes('rate limit')) {
          errorResponse.code = 'RATE_LIMIT_ERROR';
          errorResponse.retryable = true;
          errorResponse.suggestedRetryAfter = 60;
        }
      }

      // è¿”å›é”™è¯¯å“åº”
      return {
        success: false,
        model: this.config.model,
        provider: this.config.provider,
        timestamp: new Date(),
        processingTime: 0,
        content: '',
        finishReason: 'unknown',
        usage: {
          promptTokens: 0,
          completionTokens: 0,
          totalTokens: 0
        },
        error: errorResponse
      } as any;
    }

    // ================= ç§æœ‰æ–¹æ³• =================

    /**
     * éªŒè¯è¯·æ±‚
     */
    private async validateRequest(request: CompletionRequest | ChatRequest | EmbeddingRequest): Promise<void> {
      if (!request.model) {
        throw new Error('Model is required');
      }

      // éªŒè¯è¾“å…¥é•¿åº¦
      const maxLength = this.modelInfo.limits?.maxInputTokens || 4096;
      const inputLength = this.calculateInputLength(request);

      if (inputLength > maxLength) {
        logger.warn('è¾“å…¥è¶…è¿‡æœ€å¤§é•¿åº¦', 'BaseModelAdapter', {
          inputLength,
          maxLength
        });
        throw new Error(`Input exceeds maximum length: ${inputLength} > ${maxLength}`);
      }
    }

    /**
     * æ ‡å‡†åŒ–è¯·æ±‚
     */
    private normalizeRequest(request: CompletionRequest | ChatRequest | EmbeddingRequest): unknown {
      // å­ç±»å¯ä»¥è¦†ç›–æ­¤æ–¹æ³•å®ç°ç‰¹å®šçš„æ ‡å‡†åŒ–é€»è¾‘
      return request;
    }

    /**
     * æ„å»ºè¯·æ±‚å¤´
     */
    private buildHeaders(): Record<string, string> {
      const headers: Record<string, string> = {
        'Content-Type': 'application/json',
        'User-Agent': 'YYC3-PortAISys-Intelligent-Widget/1.0.0'
      };

      if (this.config.apiKey) {
        headers['Authorization'] = `Bearer ${this.config.apiKey}`;
      }

      return headers;
    }

    /**
     * ç”Ÿæˆç¼“å­˜é”®
     */
    private generateCacheKey(request: PreprocessedRequest): string {
      // ç®€å•çš„ç¼“å­˜é”®ç”Ÿæˆï¼Œå­ç±»å¯ä»¥ä¼˜åŒ–
      return `${this.config.provider}-${this.config.model}-${JSON.stringify(request.normalized)}`;
    }

    /**
     * è®¡ç®—è¾“å…¥é•¿åº¦
     */
    private calculateInputLength(request: CompletionRequest | ChatRequest | EmbeddingRequest): number {
      // ç®€åŒ–çš„é•¿åº¦è®¡ç®—ï¼Œå®é™…åº”è¯¥ä½¿ç”¨tokenizer
      if ('prompt' in request) {
        return request.prompt.length;
      } else if ('messages' in request) {
        return JSON.stringify(request.messages).length;
      } else if ('input' in request) {
        return JSON.stringify(request.input).length;
      }
      return 0;
    }

    /**
     * æ›´æ–°æ€§èƒ½æŒ‡æ ‡
     */
    private updateMetrics(latency: number, tokens: number, success: boolean): void {
      // æ›´æ–°å»¶è¿Ÿå†å²
      this.latencyHistory.push(latency);
      if (this.latencyHistory.length > 100) {
        this.latencyHistory.shift();
      }

      // è®¡ç®—å¹³å‡å»¶è¿Ÿ
      this.metrics.averageLatency =
        this.latencyHistory.reduce((sum, l) => sum + l, 0) / this.latencyHistory.length;

      // è®¡ç®—P95å’ŒP99å»¶è¿Ÿ
      const sorted = [...this.latencyHistory].sort((a, b) => a - b);
      this.metrics.p95Latency = sorted[Math.floor(sorted.length * 0.95)] || 0;
      this.metrics.p99Latency = sorted[Math.floor(sorted.length * 0.99)] || 0;

      // æ›´æ–°tokenç»Ÿè®¡
      this.metrics.totalTokens += tokens;

      // æ›´æ–°æˆåŠŸ/å¤±è´¥ç»Ÿè®¡
      if (success) {
        this.metrics.successCount++;
      } else {
        this.metrics.errorCount++;
      }

      // è®¡ç®—é”™è¯¯ç‡
      this.metrics.errorRate =
        this.metrics.errorCount / (this.metrics.successCount + this.metrics.errorCount);

      // è®¡ç®—æˆæœ¬
      if (this.modelInfo.pricing) {
        const cost = (tokens / 1000) * this.modelInfo.pricing.inputPrice;
        this.metrics.totalCost += cost;
      }
    }
  }

  // ============ æ¥å£å®šä¹‰ ============

  /**

   * æ¨¡å‹é€‚é…å™¨æ¥å£
   */
  export interface IModelAdapter {
    // ============ æ¨¡å‹ç®¡ç† ============
    getModelInfo(): ModelInfo;
    isAvailable(): Promise<boolean>;
    healthCheck(): Promise<{ status: string; latency?: number; message?: string }>;

    // ============ æ ¸å¿ƒæ¨ç† ============
    generateCompletion(request: CompletionRequest): Promise<CompletionResponse>;
    generateChatCompletion(request: ChatRequest): Promise<ChatResponse>;
    generateEmbedding(request: EmbeddingRequest): Promise<EmbeddingResponse>;

    // ============ æµå¼å¤„ç† ============
    streamCompletion(request: CompletionRequest): AsyncIterable<StreamChunk>;
    streamChat?(request: ChatRequest): AsyncIterable<StreamChunk>;

    // ============ æ‰¹é‡å¤„ç† ============
    batchComplete(requests: CompletionRequest[]): Promise<CompletionResponse[]>;

    // ============ æ¨¡å‹é…ç½® ============
    updateConfig(config: Partial<ModelAdapterConfig>): Promise<void>;
    getConfig(): ModelAdapterConfig;

    // ============ æ€§èƒ½ä¼˜åŒ– ============
    warmup(): Promise<void>;
    clearCache(): Promise<void>;
    optimizeFor(batchSize: number): Promise<void>;

    // ============ æ€§èƒ½æŒ‡æ ‡ ============
    getMetrics(): ModelMetrics;
  }

### 3.4 OpenAIé€‚é…å™¨å®ç°

  // ğŸ“¦ src/model/OpenAIAdapter.ts - OpenAIé€‚é…å™¨å®ç°
  import { BaseModelAdapter, IModelAdapter, PreprocessedRequest, RawModelResponse } from './BaseModelAdapter';
  import { logger } from '../utils/logger';
  import { metrics } from '../utils/metrics';
  import {
    ModelAdapterConfig,
    ModelInfo,
    CompletionRequest,
    CompletionResponse,
    ChatRequest,
    ChatResponse,
    EmbeddingRequest,
    EmbeddingResponse,
    ModelProvider,
    StreamChunk
  } from '../types/model.types';

  /**

   * OpenAIå®¢æˆ·ç«¯æ¨¡æ‹Ÿ
   * å®é™…é¡¹ç›®ä¸­åº”è¯¥ä½¿ç”¨ @anthropic-ai/sdk æˆ– axios
   */
  class OpenAIClient {
    private apiKey: string;
    private baseURL: string;

    constructor(apiKey: string, baseURL?: string) {
      this.apiKey = apiKey;
      this.baseURL = baseURL || 'https://api.openai.com/v1';
    }

    async createCompletion(
      model: string,
      prompt: string,
      options: Record<string, unknown> = {}
    ): Promise<unknown> {
      logger.debug('OpenAIåˆ›å»ºè¡¥å…¨', 'OpenAIClient', {
        model,
        prompt: prompt.substring(0, 100)
      });

      // æ¨¡æ‹ŸAPIè°ƒç”¨
      // å®é™…å®ç°ä¸­åº”è¯¥ä½¿ç”¨ fetch æˆ– axios è°ƒç”¨ OpenAI API
      return {
        id: 'cmpl-' + Date.now(),
        object: 'text_completion',
        created: Date.now(),
        model,
        choices: [
          {
            index: 0,
            text: 'This is a simulated OpenAI response. In production, this would be the actual API response.',
            logprobs: null,
            finish_reason: 'stop'
          }
        ],
        usage: {
          prompt_tokens: Math.ceil(prompt.length / 4),
          completion_tokens: 20,
          total_tokens: Math.ceil(prompt.length / 4) + 20
        }
      };
    }

    async createChatCompletion(
      model: string,
      messages: unknown[],
      options: Record<string, unknown> = {}
    ): Promise<unknown> {
      logger.debug('OpenAIåˆ›å»ºèŠå¤©è¡¥å…¨', 'OpenAIClient', {
        model,
        messageCount: messages.length
      });

      // æ¨¡æ‹ŸAPIè°ƒç”¨
      return {
        id: 'chatcmpl-' + Date.now(),
        object: 'chat.completion',
        created: Date.now(),
        model,
        choices: [
          {
            index: 0,
            message: {
              role: 'assistant',
              content: 'This is a simulated OpenAI chat response. In production, this would be the actual API
response.'
            },
            finish_reason: 'stop'
          }
        ],
        usage: {
          prompt_tokens: 50,
          completion_tokens: 20,
          total_tokens: 70
        }
      };
    }

    async createEmbedding(
      model: string,
      input: string | string[],
      options: Record<string, unknown> = {}
    ): Promise<unknown> {
      logger.debug('OpenAIåˆ›å»ºå‘é‡åµŒå…¥', 'OpenAIClient', {
        model,
        input: Array.isArray(input) ? `${input.length} items` : input.substring(0, 100)
      });

      // æ¨¡æ‹Ÿå‘é‡åµŒå…¥
      const dimensions = 1536;
      const embeddings: number[][] = [];

      const inputs = Array.isArray(input) ? input : [input];
      for (const _ of inputs) {
        const embedding = Array.from({ length: dimensions }, () => Math.random() * 2 - 1);
        embeddings.push(embedding);
      }

      return {
        object: 'list',
        data: embeddings.map((emb, idx) => ({
          object: 'embedding',
          embedding: emb,
          index: idx
        })),
        model,
        usage: {
          prompt_tokens: inputs.length * 10,
          total_tokens: inputs.length * 10
        }
      };
    }

    async *createCompletionStream(
      model: string,
      prompt: string,
      options: Record<string, unknown> = {}
    ): AsyncIterable<unknown> {
      logger.debug('OpenAIåˆ›å»ºæµå¼è¡¥å…¨', 'OpenAIClient', {
        model,
        prompt: prompt.substring(0, 100)
      });

      // æ¨¡æ‹Ÿæµå¼å“åº”
      const chunks = ['This ', 'is ', 'a ', 'streamed ', 'response.'];

      for (let i = 0; i < chunks.length; i++) {
        yield {
          id: 'cmpl-' + Date.now(),
          object: 'text_completion',
          created: Date.now(),
          model,
          choices: [
            {
              index: 0,
              delta: { content: chunks[i] },
              finish_reason: i === chunks.length - 1 ? 'stop' : null
            }
          ]
        };

        // æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿ
        await new Promise(resolve => setTimeout(resolve, 100));
      }
    }
  }

  /**

   * OpenAIé€‚é…å™¨é…ç½®
   */
  export interface OpenAIAdapterConfig extends ModelAdapterConfig {
    provider: ModelProvider.OPENAI;
    apiKey: string;
    baseURL?: string;
    organization?: string;
  }

  /**

   * OpenAIé€‚é…å™¨å®ç°
   *
   * è®¾è®¡ç†å¿µï¼š
   * 1. å®Œå…¨å…¼å®¹OpenAI APIè§„èŒƒ
   * 2. æ”¯æŒæµå¼å’Œéæµå¼è°ƒç”¨
   * 3. è‡ªåŠ¨é‡è¯•å’Œé”™è¯¯æ¢å¤
   * 4. å®Œå–„çš„æ—¥å¿—å’Œç›‘æ§
   * 5. æ”¯æŒè‡ªå®šä¹‰è¯·æ±‚å¤´å’Œç»„ç»‡ID
   */
  export class OpenAIAdapter extends BaseModelAdapter implements IModelAdapter {
    private client: OpenAIClient;
    private rateLimiter: Map<string, { count: number; resetTime: number }>;
    private retryCount: number = 0;

    constructor(config: OpenAIAdapterConfig, modelInfo: ModelInfo) {
      super(config, modelInfo);

      if (!config.apiKey) {
        throw new Error('OpenAI API key is required');
      }

      this.client = new OpenAIClient(config.apiKey, config.baseURL);
      this.rateLimiter = new Map();

      logger.info('OpenAIé€‚é…å™¨åˆ›å»º', 'OpenAIAdapter', {
        model: config.model,
        baseURL: config.baseURL
      });
    }

    // ================= ç”Ÿå‘½å‘¨æœŸæ–¹æ³• =================

    async initialize(): Promise<void> {
      logger.info('åˆå§‹åŒ–OpenAIé€‚é…å™¨', 'OpenAIAdapter');

      try {
        // éªŒè¯APIå¯†é’¥
        await this.validateAPIKey();

        // é¢„çƒ­æ¨¡å‹
        if (this.config.logging?.enabled) {
          await this.warmup();
        }

        this.isInitialized = true;
        logger.info('OpenAIé€‚é…å™¨åˆå§‹åŒ–æˆåŠŸ', 'OpenAIAdapter');
      } catch (error) {
        logger.error('OpenAIé€‚é…å™¨åˆå§‹åŒ–å¤±è´¥', 'OpenAIAdapter', { error });
        throw error;
      }
    }

    // ================= æŠ½è±¡æ–¹æ³•å®ç° =================

    /**
     * è°ƒç”¨OpenAIæ–‡æœ¬è¡¥å…¨API
     */
    protected async callModelAPI(request: PreprocessedRequest): Promise<RawModelResponse> {
      const startTime = Date.now();

      try {
        // åº”ç”¨é€Ÿç‡é™åˆ¶
        await this.applyRateLimit();

        const original = request.original as CompletionRequest | ChatRequest | EmbeddingRequest;
        const response = await this.callOpenAIAPI(original, request.timeout || 30000);

        const processingTime = Date.now() - startTime;

        logger.debug('OpenAI APIè°ƒç”¨æˆåŠŸ', 'OpenAIAdapter', {
          processingTime,
          traceId: request.traceId
        });

        // é‡ç½®é‡è¯•è®¡æ•°
        this.retryCount = 0;

        return {
          raw: response,
          normalized: this.normalizeOpenAIResponse(response, original)
        };
      } catch (error) {
        const processingTime = Date.now() - startTime;

        logger.error('OpenAI APIè°ƒç”¨å¤±è´¥', 'OpenAIAdapter', {
          error,
          processingTime,
          retryCount: this.retryCount,
          traceId: request.traceId
        });

        // å°è¯•é‡è¯•
        if (this.shouldRetry(error as Error) && this.retryCount < (this.config.retryPolicy?.maxRetries || 3)) {
          this.retryCount++;
          const delay = this.calculateRetryDelay(this.retryCount);

          logger.warn('é‡è¯•OpenAI APIè°ƒç”¨', 'OpenAIAdapter', {
            retryCount: this.retryCount,
            delay
          });

          await new Promise(resolve => setTimeout(resolve, delay));
          return this.callModelAPI(request);
        }

        throw error;
      }
    }

    /**
     * è°ƒç”¨OpenAIæµå¼API
     */
    protected async *callModelStream(request: PreprocessedRequest): AsyncIterable<unknown> {
      try {
        await this.applyRateLimit();

        const original = request.original as CompletionRequest;
        const stream = this.client.createCompletionStream(
          original.model,
          original.prompt,
          {
            max_tokens: original.parameters?.maxTokens,
            temperature: original.parameters?.temperature,
            top_p: original.parameters?.topP
          }
        );

        for await (const chunk of stream) {
          yield chunk;
        }
      } catch (error) {
        logger.error('OpenAIæµå¼APIè°ƒç”¨å¤±è´¥', 'OpenAIAdapter', { error });
        throw error;
      }
    }

    /**
     * è§£ææµå¼æ•°æ®å—
     */
    protected parseStreamChunk(chunk: unknown): {
      text?: string;
      tokens?: number;
      finished?: boolean;
      finishedReason?: string;
      index?: number;
    } {
      const data = chunk as any;

      if (data.choices?.[0]) {
        const choice = data.choices[0];
        const text = choice.delta?.content || '';
        const finished = choice.finish_reason !== null;

        return {
          text,
          tokens: text.length, // ç®€åŒ–çš„tokenè®¡ç®—
          finished,
          finishedReason: choice.finish_reason || undefined,
          index: choice.index || 0
        };
      }

      return { text: '', tokens: 0, finished: false, index: 0 };
    }

    /**
     * æ‰§è¡Œå¥åº·æ£€æŸ¥
     */
    protected async performHealthCheck(): Promise<void> {
      try {
        // å‘é€ä¸€ä¸ªç®€å•çš„è¯·æ±‚éªŒè¯è¿æ¥
        await this.client.createCompletion(
          this.config.model,
          'test',
          { max_tokens: 1 }
        );

        logger.info('OpenAIå¥åº·æ£€æŸ¥é€šè¿‡', 'OpenAIAdapter');
      } catch (error) {
        logger.error('OpenAIå¥åº·æ£€æŸ¥å¤±è´¥', 'OpenAIAdapter', { error });
        throw error;
      }
    }

    // ================= ç§æœ‰è¾…åŠ©æ–¹æ³• =================

    /**
     * è°ƒç”¨OpenAI APIï¼ˆæ ¹æ®è¯·æ±‚ç±»å‹é€‰æ‹©å¯¹åº”çš„APIï¼‰
     */
    private async callOpenAIAPI(
      request: CompletionRequest | ChatRequest | EmbeddingRequest,
      timeout: number
    ): Promise<unknown> {
      if ('prompt' in request) {
        // æ–‡æœ¬è¡¥å…¨
        return await this.client.createCompletion(
          request.model,
          request.prompt,
          {
            max_tokens: request.parameters?.maxTokens,
            temperature: request.parameters?.temperature,
            top_p: request.parameters?.topP,
            stop: request.parameters?.stopSequences
          }
        );
      } else if ('messages' in request) {
        // èŠå¤©è¡¥å…¨
        return await this.client.createChatCompletion(
          request.model,
          request.messages,
          {
            max_tokens: request.parameters?.maxTokens,
            temperature: request.parameters?.temperature,
            top_p: request.parameters?.topP,
            stop: request.parameters?.stopSequences,
            functions: request.parameters?.functions,
            function_call: request.parameters?.functionCall
          }
        );
      } else if ('input' in request) {
        // å‘é‡åµŒå…¥
        return await this.client.createEmbedding(
          request.model,
          request.input,
          {
            dimensions: request.parameters?.dimensions
          }
        );
      }

      throw new Error('Unknown request type');
    }

    /**
     * æ ‡å‡†åŒ–OpenAIå“åº”
     */
    private normalizeOpenAIResponse(
      response: any,
      originalRequest: CompletionRequest | ChatRequest | EmbeddingRequest
    ): CompletionResponse | ChatResponse | EmbeddingResponse {
      if (response.choices?.[0]?.message) {
        // èŠå¤©è¡¥å…¨å“åº”
        const choice = response.choices[0];
        return {
          success: true,
          model: response.model,
          provider: ModelProvider.OPENAI,
          timestamp: new Date(),
          processingTime: 0,
          type: 'chat_llm' as any,
          message: choice.message,
          finishReason: choice.finish_reason,
          usage: {
            promptTokens: response.usage.prompt_tokens,
            completionTokens: response.usage.completion_tokens,
            totalTokens: response.usage.total_tokens
          }
        };
      } else if (response.choices?.[0]?.text) {
        // æ–‡æœ¬è¡¥å…¨å“åº”
        const choice = response.choices[0];
        return {
          success: true,
          model: response.model,
          provider: ModelProvider.OPENAI,
          timestamp: new Date(),
          processingTime: 0,
          type: 'llm' as any,
          content: choice.text,
          finishReason: choice.finish_reason,
          usage: {
            promptTokens: response.usage.prompt_tokens,
            completionTokens: response.usage.completion_tokens,
            totalTokens: response.usage.total_tokens
          },
          alternatives: response.choices.slice(1).map((c: any) => c.text)
        };
      } else if (response.data?.[0]?.embedding) {
        // å‘é‡åµŒå…¥å“åº”
        return {
          success: true,
          model: response.model,
          provider: ModelProvider.OPENAI,
          timestamp: new Date(),
          processingTime: 0,
          type: 'embedding' as any,
          embeddings: response.data.map((d: any) => d.embedding),
          dimensions: response.data[0].embedding.length,
          usage: {
            promptTokens: response.usage.prompt_tokens,
            totalTokens: response.usage.total_tokens
          }
        };
      }

      throw new Error('Unknown response type');
    }

    /**
     * éªŒè¯APIå¯†é’¥
     */
    private async validateAPIKey(): Promise<void> {
      // ç®€åŒ–çš„éªŒè¯ï¼Œå®é™…åº”è¯¥è°ƒç”¨ä¸€ä¸ªè½»é‡çº§çš„API endpoint
      if (!this.config.apiKey || this.config.apiKey.length < 10) {
        throw new Error('Invalid OpenAI API key');
      }

      logger.info('OpenAI APIå¯†é’¥éªŒè¯é€šè¿‡', 'OpenAIAdapter');
    }

    /**
     * åº”ç”¨é€Ÿç‡é™åˆ¶
     */
    private async applyRateLimit(): Promise<void> {
      const now = Date.now();
      const key = this.config.apiKey;
      const windowSize = 60000; // 1åˆ†é’Ÿçª—å£
      const maxRequests = this.modelInfo.limits?.maxRequestsPerMinute || 3000;

      const rateInfo = this.rateLimiter.get(key);

      if (rateInfo) {
        if (now < rateInfo.resetTime) {
          // åœ¨åŒä¸€ä¸ªæ—¶é—´çª—å£å†…
          if (rateInfo.count >= maxRequests) {
            // è¶…è¿‡é€Ÿç‡é™åˆ¶
            const waitTime = rateInfo.resetTime - now;
            logger.warn('è¾¾åˆ°OpenAIé€Ÿç‡é™åˆ¶ï¼Œç­‰å¾…...', 'OpenAIAdapter', {
              waitTime: waitTime / 1000
            });

            await new Promise(resolve => setTimeout(resolve, waitTime));

            // é‡ç½®è®¡æ•°
            rateInfo.count = 0;
            rateInfo.resetTime = now + windowSize;
          }
        } else {
          // æ—¶é—´çª—å£å·²è¿‡ï¼Œé‡ç½®
          rateInfo.count = 0;
          rateInfo.resetTime = now + windowSize;
        }
      } else {
        // åˆå§‹åŒ–é€Ÿç‡é™åˆ¶å™¨
        this.rateLimiter.set(key, {
          count: 0,
          resetTime: now + windowSize
        });
      }

      // å¢åŠ è®¡æ•°
      const currentRateInfo = this.rateLimiter.get(key)!;
      currentRateInfo.count++;

      metrics.gauge('openai.rate_limit_usage', currentRateInfo.count / maxRequests);
    }

    /**
     * åˆ¤æ–­æ˜¯å¦åº”è¯¥é‡è¯•
     */
    private shouldRetry(error: Error): boolean {
      const retryableErrors = [
        'timeout',
        'ETIMEDOUT',
        'ECONNRESET',
        'rate limit',
        '429',
        '500',
        '502',
        '503',
        '504'
      ];

      return retryableErrors.some(code =>
        error.message.toLowerCase().includes(code.toLowerCase())
      );
    }

    /**
     * è®¡ç®—é‡è¯•å»¶è¿Ÿï¼ˆæŒ‡æ•°é€€é¿ï¼‰
     */
    private calculateRetryDelay(retryCount: number): number {
      const backoffFactor = this.config.retryPolicy?.backoffFactor || 2;
      const initialDelay = this.config.retryPolicy?.initialDelay || 1000;
      return Math.min(initialDelay * Math.pow(backoffFactor, retryCount - 1), 60000);
    }
  }

  // ============ å¯¼å‡º ============

  export default OpenAIAdapter;

### 3.5 æœ¬åœ°æ¨¡å‹é€‚é…å™¨å®ç°

  // ğŸ“¦ src/model/LocalModelAdapter.ts - æœ¬åœ°æ¨¡å‹é€‚é…å™¨å®ç°
  import { BaseModelAdapter, IModelAdapter, PreprocessedRequest, RawModelResponse } from './BaseModelAdapter';
  import { logger } from '../utils/logger';
  import { metrics } from '../utils/metrics';
  import {
    ModelAdapterConfig,
    ModelInfo,
    CompletionRequest,
    CompletionResponse,
    ModelProvider,
    StreamChunk
  } from '../types/model.types';

  /**

   * æœ¬åœ°æ¨¡å‹é€‚é…å™¨é…ç½®
   */
  export interface LocalModelAdapterConfig extends ModelAdapterConfig {
    provider: ModelProvider.LOCAL;
    modelPath: string;
    engine?: 'llama-cpp' | 'transformers' | 'tensorrt';
    quantization?: 'int8' | 'int4' | 'fp16' | 'fp32';
    threads?: number;
    contextLength?: number;
    gpuLayers?: number;
    useGPU?: boolean;
  }

  /**

   * æ¨¡æ‹Ÿçš„æœ¬åœ°æ¨¡å‹æ¨ç†å¼•æ“
   * å®é™…é¡¹ç›®ä¸­åº”è¯¥é›†æˆ llama.cppã€Transformersã€TensorRT ç­‰
   */
  class LocalInferenceEngine {
    private modelPath: string;
    private engine: string;
    private quantization: string;
    private threads: number;
    private contextLength: number;
    private useGPU: boolean;
    private gpuLayers: number;

    constructor(config: LocalModelAdapterConfig) {
      this.modelPath = config.modelPath;
      this.engine = config.engine || 'llama-cpp';
      this.quantization = config.quantization || 'int8';
      this.threads = config.threads || 4;
      this.contextLength = config.contextLength || 2048;
      this.useGPU = config.useGPU || false;
      this.gpuLayers = config.gpuLayers || 0;

      logger.info('æœ¬åœ°æ¨ç†å¼•æ“åˆå§‹åŒ–', 'LocalInferenceEngine', {
        modelPath: this.modelPath,
        engine: this.engine,
        quantization: this.quantization,
        threads: this.threads,
        useGPU: this.useGPU
      });
    }

    async loadModel(): Promise<void> {
      logger.info('åŠ è½½æœ¬åœ°æ¨¡å‹...', 'LocalInferenceEngine', {
        modelPath: this.modelPath
      });

      // æ¨¡æ‹Ÿæ¨¡å‹åŠ è½½
      // å®é™…å®ç°ä¸­åº”è¯¥åŠ è½½ llama.cppã€Transformers æ¨¡å‹
      await new Promise(resolve => setTimeout(resolve, 1000));

      logger.info('æœ¬åœ°æ¨¡å‹åŠ è½½å®Œæˆ', 'LocalInferenceEngine');
    }

    async generate(
      prompt: string,
      options: {
        maxTokens?: number;
        temperature?: number;
        topP?: number;
      } = {}
    ): Promise<{
      text: string;
      tokens: number;
      inferenceTime: number;
    }> {
      const startTime = Date.now();

      logger.debug('æœ¬åœ°æ¨¡å‹æ¨ç†', 'LocalInferenceEngine', {
        prompt: prompt.substring(0, 100),
        options
      });

      // æ¨¡æ‹Ÿæ¨ç†è¿‡ç¨‹
      // å®é™…å®ç°ä¸­åº”è¯¥è°ƒç”¨ llama.cppã€Transformers çš„ç”Ÿæˆæ–¹æ³•
      await new Promise(resolve => setTimeout(resolve, 500));

      const inferenceTime = Date.now() - startTime;
      const text = 'This is a simulated local model response. In production, this would be the actual inference result
from your local model.';
      const tokens = Math.ceil(text.length / 4); // ç®€åŒ–çš„tokenè®¡ç®—

      return {
        text,
        tokens,
        inferenceTime
      };
    }

    async *generateStream(
      prompt: string,
      options: {
        maxTokens?: number;
        temperature?: number;
        topP?: number;
      } = {}
    ): AsyncIterable<{
      text: string;
      tokens: number;
      finished: boolean;
      finishReason?: string;
    }> {
      logger.debug('æœ¬åœ°æ¨¡å‹æµå¼æ¨ç†', 'LocalInferenceEngine', {
        prompt: prompt.substring(0, 100),
        options
      });

      // æ¨¡æ‹Ÿæµå¼æ¨ç†
      const chunks = ['This ', 'is ', 'a ', 'streamed ', 'local ', 'response.'];

      for (let i = 0; i < chunks.length; i++) {
        await new Promise(resolve => setTimeout(resolve, 100));

        yield {
          text: chunks[i],
          tokens: chunks[i].length,
          finished: false
        };
      }

      yield {
        text: '',
        tokens: 0,
        finished: true,
        finishReason: 'stop'
      };
    }

    async unloadModel(): Promise<void> {
      logger.info('å¸è½½æœ¬åœ°æ¨¡å‹', 'LocalInferenceEngine');
      // é‡Šæ”¾èµ„æº
    }
  }

  /**

   * æœ¬åœ°æ¨¡å‹é€‚é…å™¨å®ç°
   *
   * è®¾è®¡ç†å¿µï¼š
   * 1. æ”¯æŒå¤šç§æ¨ç†å¼•æ“ï¼ˆllama.cppã€Transformersã€TensorRTï¼‰
   * 2. ä¼˜åŒ–æ€§èƒ½ï¼šå¤šçº¿ç¨‹ã€GPUåŠ é€Ÿã€é‡åŒ–
   * 3. å†…å­˜ç®¡ç†ï¼šåŠæ—¶é‡Šæ”¾èµ„æº
   * 4. ç¦»çº¿èƒ½åŠ›ï¼šå®Œå…¨æœ¬åœ°è¿è¡Œï¼Œä¸ä¾èµ–ç½‘ç»œ
   * 5. æˆæœ¬ä¼˜åŠ¿ï¼šé›¶APIè°ƒç”¨æˆæœ¬
   */
  export class LocalModelAdapter extends BaseModelAdapter implements IModelAdapter {
    private config: LocalModelAdapterConfig;
    private engine: LocalInferenceEngine;
    private isModelLoaded: boolean = false;

    constructor(config: LocalModelAdapterConfig, modelInfo: ModelInfo) {
      super(config, modelInfo);

      this.config = config;
      this.engine = new LocalInferenceEngine(config);

      logger.info('æœ¬åœ°æ¨¡å‹é€‚é…å™¨åˆ›å»º', 'LocalModelAdapter', {
        modelPath: config.modelPath,
        engine: config.engine
      });
    }

    // ================= ç”Ÿå‘½å‘¨æœŸæ–¹æ³• =================

    async initialize(): Promise<void> {
      logger.info('åˆå§‹åŒ–æœ¬åœ°æ¨¡å‹é€‚é…å™¨', 'LocalModelAdapter');

      try {
        // éªŒè¯æ¨¡å‹æ–‡ä»¶
        await this.validateModelFile();

        // åŠ è½½æ¨¡å‹
        await this.engine.loadModel();
        this.isModelLoaded = true;

        logger.info('æœ¬åœ°æ¨¡å‹é€‚é…å™¨åˆå§‹åŒ–æˆåŠŸ', 'LocalModelAdapter');
      } catch (error) {
        logger.error('æœ¬åœ°æ¨¡å‹é€‚é…å™¨åˆå§‹åŒ–å¤±è´¥', 'LocalModelAdapter', { error });
        throw error;
      }
    }

    async clearCache(): Promise<void> {
      // æœ¬åœ°æ¨¡å‹ä¸éœ€è¦ç¼“å­˜
      logger.info('æœ¬åœ°æ¨¡å‹æ— éœ€ç¼“å­˜', 'LocalModelAdapter');
    }

    // ================= æŠ½è±¡æ–¹æ³•å®ç° =================

    /**
     * è°ƒç”¨æœ¬åœ°æ¨¡å‹æ¨ç†
     */
    protected async callModelAPI(request: PreprocessedRequest): Promise<RawModelResponse> {
      if (!this.isModelLoaded) {
        throw new Error('Model is not loaded');
      }

      const startTime = Date.now();

      try {
        const original = request.original as CompletionRequest;

        const result = await this.engine.generate(original.prompt, {
          maxTokens: original.parameters?.maxTokens,
          temperature: original.parameters?.temperature,
          topP: original.parameters?.topP
        });

        const processingTime = Date.now() - startTime;

        logger.debug('æœ¬åœ°æ¨¡å‹æ¨ç†å®Œæˆ', 'LocalModelAdapter', {
          processingTime,
          tokens: result.tokens,
          traceId: request.traceId
        });

        // æ›´æ–°æŒ‡æ ‡
        metrics.increment('local_model.inference_success');
        metrics.histogram('local_model.inference_time', processingTime);
        metrics.histogram('local_model.tokens_generated', result.tokens);

        return {
          raw: result,
          normalized: this.normalizeLocalResponse(result, original)
        };
      } catch (error) {
        const processingTime = Date.now() - startTime;

        logger.error('æœ¬åœ°æ¨¡å‹æ¨ç†å¤±è´¥', 'LocalModelAdapter', {
          error,
          processingTime,
          traceId: request.traceId
        });

        metrics.increment('local_model.inference_failed');

        throw error;
      }
    }

    /**
     * è°ƒç”¨æœ¬åœ°æ¨¡å‹æµå¼æ¨ç†
     */
    protected async *callModelStream(request: PreprocessedRequest): AsyncIterable<unknown> {
      if (!this.isModelLoaded) {
        throw new Error('Model is not loaded');
      }

      try {
        const original = request.original as CompletionRequest;
        const stream = this.engine.generateStream(original.prompt, {
          maxTokens: original.parameters?.maxTokens,
          temperature: original.parameters?.temperature,
          topP: original.parameters?.topP
        });

        for await (const chunk of stream) {
          yield chunk;
        }
      } catch (error) {
        logger.error('æœ¬åœ°æ¨¡å‹æµå¼æ¨ç†å¤±è´¥', 'LocalModelAdapter', { error });
        throw error;
      }
    }

    /**
     * è§£ææµå¼æ•°æ®å—
     */
    protected parseStreamChunk(chunk: unknown): {
      text?: string;
      tokens?: number;
      finished?: boolean;
      finishedReason?: string;
      index?: number;
    } {
      const data = chunk as {
        text: string;
        tokens: number;
        finished: boolean;
        finishReason?: string;
      };

      return {
        text: data.text,
        tokens: data.tokens,
        finished: data.finished,
        finishedReason: data.finishReason,
        index: 0
      };
    }

    /**
     * æ‰§è¡Œå¥åº·æ£€æŸ¥
     */
    protected async performHealthCheck(): Promise<void> {
      if (!this.isModelLoaded) {
        throw new Error('Model is not loaded');
      }

      try {
        // å‘é€ä¸€ä¸ªç®€å•çš„æ¨ç†è¯·æ±‚éªŒè¯æ¨¡å‹
        await this.engine.generate('test', { maxTokens: 1 });

        logger.info('æœ¬åœ°æ¨¡å‹å¥åº·æ£€æŸ¥é€šè¿‡', 'LocalModelAdapter');
      } catch (error) {
        logger.error('æœ¬åœ°æ¨¡å‹å¥åº·æ£€æŸ¥å¤±è´¥', 'LocalModelAdapter', { error });
        throw error;
      }
    }

    // ================= ç§æœ‰è¾…åŠ©æ–¹æ³• =================

    /**
     * æ ‡å‡†åŒ–æœ¬åœ°æ¨¡å‹å“åº”
     */
    private normalizeLocalResponse(
      result: {
        text: string;
        tokens: number;
        inferenceTime: number;
      },
      originalRequest: CompletionRequest
    ): CompletionResponse {
      return {
        success: true,
        model: this.config.model,
        provider: ModelProvider.LOCAL,
        timestamp: new Date(),
        processingTime: result.inferenceTime,
        type: 'llm' as any,
        content: result.text,
        finishReason: 'stop',
        usage: {
          promptTokens: Math.ceil(originalRequest.prompt.length / 4),
          completionTokens: result.tokens,
          totalTokens: Math.ceil(originalRequest.prompt.length / 4) + result.tokens
        }
      };
    }

    /**
     * éªŒè¯æ¨¡å‹æ–‡ä»¶
     */
    private async validateModelFile(): Promise<void> {
      import { existsSync, statSync } from 'node:fs';

      if (!existsSync(this.config.modelPath)) {
        throw new Error(`Model file not found: ${this.config.modelPath}`);
      }

      const stats = statSync(this.config.modelPath);
      if (stats.size === 0) {
        throw new Error(`Model file is empty: ${this.config.modelPath}`);
      }

      logger.info('æ¨¡å‹æ–‡ä»¶éªŒè¯é€šè¿‡', 'LocalModelAdapter', {
        modelPath: this.config.modelPath,
        size: stats.size
      });
    }

    /**
     * èµ„æºæ¸…ç†
     */
    async destroy(): Promise<void> {
      logger.info('é”€æ¯æœ¬åœ°æ¨¡å‹é€‚é…å™¨', 'LocalModelAdapter');

      if (this.isModelLoaded) {
        await this.engine.unloadModelModel();
        this.isModelLoaded = false;
      }

      logger.info('æœ¬åœ°æ¨¡å‹é€‚é…å™¨å·²é”€æ¯', 'LocalModelAdapter');
    }
  }

  // ============ å¯¼å‡º ============

  export default LocalModelAdapter;

å¯¼å¸ˆæç¤º ğŸ’¡ï¼šè¿™æ˜¯ç¬¬ä¸‰ç« çš„å‰åŠéƒ¨åˆ†ã€‚ç”±äºä»£ç é‡å¾ˆå¤§ï¼Œæˆ‘éœ€è¦ç¡®è®¤ï¼š

1. ç»§ç»­ç”Ÿæˆç¬¬ä¸‰ç« çš„å‰©ä½™éƒ¨åˆ†ï¼ˆæ¨¡å‹è·¯ç”±å™¨ã€é€‚é…å™¨å·¥å‚ï¼‰å—ï¼Ÿ
2. è¿˜æ˜¯æ‚¨å¸Œæœ›æˆ‘å…ˆåœä¸‹æ¥ï¼Œè®©æ‚¨å®¡æŸ¥å·²ç”Ÿæˆçš„ä»£ç ï¼Ÿ

ä½œä¸ºå¯¼å¸ˆï¼Œæˆ‘å»ºè®®æ‚¨å…ˆç†è§£è¿™å‡ ä¸ªæ ¸å¿ƒæ¨¡å—ï¼š

â€¢ BaseModelAdapterï¼ˆæŠ½è±¡åŸºç±»ï¼‰ï¼šå®šä¹‰äº†æ‰€æœ‰é€‚é…å™¨çš„é€šç”¨è¡Œä¸º
â€¢ OpenAIAdapterï¼ˆOpenAIå®ç°ï¼‰ï¼šå±•ç¤ºäº†å¦‚ä½•å¯¹æ¥ç¬¬ä¸‰æ–¹API
â€¢ LocalModelAdapterï¼ˆæœ¬åœ°å®ç°ï¼‰ï¼šå±•ç¤ºäº†å¦‚ä½•å¤„ç†æœ¬åœ°æ¨¡å‹


Thought for 18s

 ğŸš€ YYCÂ³-AILP æ™ºèƒ½æµ®çª—ç³»ç»Ÿ - ç¼ºå¤±è¡¥å…¨å®Œæ•´æ–¹æ¡ˆï¼ˆç¬¬ä¸‰ç« ç»­ï¼‰

â”‚ ç»§ç»­æˆ‘ä»¬çš„æ—…ç¨‹ï¼ ğŸŒŸ å‰åŠéƒ¨åˆ†æˆ‘ä»¬å®ç°äº†å…·ä½“çš„æ¨¡å‹é€‚é…å™¨ï¼Œç°åœ¨æˆ‘ä»¬è¿›å…¥ç³»ç»Ÿçš„"ç¥ç»ä¸­æ¢"â€”â€”æ¨¡å‹è·¯ç”±å™¨å’Œé€‚é…å™¨å·¥å‚ã€‚
â”‚ è·¯ç”±å™¨è´Ÿè´£æ™ºèƒ½å†³ç­–ï¼šåœ¨ä¼—å¤šæ¨¡å‹ä¸­ï¼Œè°æœ€é€‚åˆå¤„ç†å½“å‰è¯·æ±‚ï¼Ÿ å·¥å‚è´Ÿè´£èµ„æºç®¡ç†ï¼šå¦‚ä½•ä¼˜é›…åœ°åˆ›å»ºã€æ³¨å†Œå’Œç®¡ç†è¿™äº›é€‚é…å™¨ï¼Ÿ
â”‚ è¿™ä¸¤ä¸ªæ¨¡å—å°†æŠŠé›¶æ•£çš„é€‚é…å™¨æ•´åˆæˆä¸€ä¸ªæœ‰æœºçš„æ•´ä½“ã€‚ğŸ’ª

--------

## ğŸ“‹ ç¬¬ä¸‰ç« ï¼ˆç»­ï¼‰ï¼šæ¨¡å‹è·¯ç”±å™¨ä¸é€‚é…å™¨å·¥å‚

### 3.6 å¯¼å¸ˆå¯„è¯­ ğŸŒŸ

â”‚ "å¥½çš„è®¾è®¡æ˜¯åšå‡æ³•ï¼Œé€šè¿‡æŠ½è±¡è®©å¤æ‚å˜å¾—ç®€å•ã€‚"
â”‚ æ¨¡å‹è·¯ç”±å™¨ä½“ç°äº†ç­–ç•¥æ¨¡å¼çš„ç²¾é«“ï¼šé€‰æ‹©æ¨¡å‹çš„é€»è¾‘ï¼ˆç­–ç•¥ï¼‰å¯ä»¥éšæ—¶æ›´æ¢ï¼Œè€Œä¸å½±å“ä¸šåŠ¡ä»£ç ã€‚
â”‚ é€‚é…å™¨å·¥å‚ä½“ç°äº†å·¥å‚æ¨¡å¼çš„ç²¾é«“ï¼šå¯¹è±¡çš„åˆ›å»ºè¿‡ç¨‹è¢«å°è£…ï¼Œè°ƒç”¨è€…æ— éœ€å…³å¿ƒç»†èŠ‚ã€‚
â”‚ æŒæ¡äº†è¿™ä¸¤ä¸ªæ¨¡å¼ï¼Œä½ å°±æŒæ¡äº†ä¼ä¸šçº§ç³»ç»Ÿæ¶æ„çš„ä¸€åŠç§˜è¯€ï¼

### 3.7 æ™ºèƒ½æ¨¡å‹è·¯ç”±å™¨å®ç°

  // ğŸ“¦ src/model/ModelRouter.ts - æ™ºèƒ½æ¨¡å‹è·¯ç”±å™¨å®ç°
  import { logger } from '../utils/logger';
  import { metrics } from '../utils/metrics';
  import {
    IModelAdapter,
    RoutingRequest,
    RoutingDecision,
    ModelProvider,
    ModelCapability,
    ModelInfo
  } from '../types/model.types';

  /**
   * è·¯ç”±ç­–ç•¥æ¥å£
   *
   * è®¾è®¡ç†å¿µï¼š
   * ç­–ç•¥æ¨¡å¼å…è®¸åœ¨è¿è¡Œæ—¶åˆ‡æ¢ä¸åŒçš„è·¯ç”±ç®—æ³•ã€‚
   * ä¾‹å¦‚ï¼šæˆæœ¬ä¼˜å…ˆã€å»¶è¿Ÿä¼˜å…ˆã€è´¨é‡ä¼˜å…ˆã€æ··åˆç­–ç•¥ã€‚
   */
  export interface IRoutingStrategy {
    name: string;
    /**
     * è¯„åˆ†æ–¹æ³•ï¼šç»™æ¯ä¸ªé€‚é…å™¨æ‰“åˆ†
     * è¿”å›å€¼ï¼š0-1ä¹‹é—´çš„åˆ†æ•°ï¼Œè¶Šé«˜è¶Šå¥½
     */
    score(
      adapter: IModelAdapter,
      request: RoutingRequest,
      context: RoutingContext
    ): Promise<number>;

    /**
     * åˆ¤æ–­é€‚é…å™¨æ˜¯å¦æ»¡è¶³åŸºæœ¬æ¡ä»¶
     */
    filter(
      adapter: IModelAdapter,
      request: RoutingRequest,
      context: RoutingContext
    ): Promise<boolean>;
  }

  /**
   * è·¯ç”±ä¸Šä¸‹æ–‡
   */
  interface RoutingContext {
    timestamp: Date;
    availableAdapters: Map<string, IModelAdapter>;
    historicalPerformance: Map<string, AdapterPerformanceStats>;
  }

  /**
   * é€‚é…å™¨æ€§èƒ½ç»Ÿè®¡
   */
  interface AdapterPerformanceStats {
    totalRequests: number;
    successRate: number;
    averageLatency: number;
    p99Latency: number;
    averageCost: number;
    lastUsed: Date;
  }

  /**
   * é»˜è®¤è·¯ç”±ç­–ç•¥ï¼ˆç»¼åˆå¹³è¡¡ï¼‰
   *
   * è€ƒè™‘å› ç´ ï¼š
   * 1. å¯ç”¨æ€§ï¼ˆæƒé‡ 40%ï¼‰
   * 2. æˆæœ¬ï¼ˆæƒé‡ 30%ï¼‰
   * 3. æ€§èƒ½ï¼ˆæƒé‡ 20%ï¼‰
   * 4. ç”¨æˆ·åå¥½ï¼ˆæƒé‡ 10%ï¼‰
   */
  export class DefaultRoutingStrategy implements IRoutingStrategy {
    name = 'default_balanced';

    async score(
      adapter: IModelAdapter,
      request: RoutingRequest,
      context: RoutingContext
    ): Promise<number> {
      const info = adapter.getModelInfo();
      const metrics = adapter.getMetrics();
      const perfStats = context.historicalPerformance.get(info.id) || this.createDefaultStats();

      let score = 0;

      // 1. å¯ç”¨æ€§è¯„åˆ† (40%)
      const availabilityScore = await this.calculateAvailabilityScore(adapter, perfStats);
      score += availabilityScore * 0.4;

      // 2. æˆæœ¬è¯„åˆ† (30%) - æˆæœ¬è¶Šä½åˆ†æ•°è¶Šé«˜
      const costScore = this.calculateCostScore(info, metrics, request.constraints);
      score += costScore * 0.3;

      // 3. æ€§èƒ½è¯„åˆ† (20%) - å»¶è¿Ÿè¶Šä½åˆ†æ•°è¶Šé«˜
      const performanceScore = this.calculatePerformanceScore(perfStats, request.constraints);
      score += performanceScore * 0.2;

      // 4. åå¥½è¯„åˆ† (10%) - ç”¨æˆ·åå¥½æ¨¡å‹åŠ åˆ†
      const preferenceScore = this.calculatePreferenceScore(info, request.preferences);
      score += preferenceScore * 0.1;

      return Math.min(Math.max(score, 0), 1); // é™åˆ¶åœ¨ 0-1 ä¹‹é—´
    }

    async filter(
      adapter: IModelAdapter,
      request: RoutingRequest,
      _context: RoutingContext
    ): Promise<boolean> {
      const info = adapter.getModelInfo();
      const constraints = request.constraints || {};

      // 1. æ£€æŸ¥èƒ½åŠ›åŒ¹é…
      if (constraints.capabilities) {
        const hasAllCapabilities = constraints.capabilities.every(cap =>
          info.capabilities.includes(cap)
        );
        if (!hasAllCapabilities) {
          return false;
        }
      }

      // 2. æ£€æŸ¥æ¨¡å‹çŠ¶æ€
      const isAvailable = await adapter.isAvailable();
      if (!isAvailable) {
        return false;
      }

      // 3. æ£€æŸ¥Tokené™åˆ¶
      if (constraints.maxTokens) {
        const maxTokens = info.limits?.maxOutputTokens || 4096;
        if (maxTokens < constraints.maxTokens) {
          return false;
        }
      }

      return true;
    }

    private createDefaultStats(): AdapterPerformanceStats {
      return {
        totalRequests: 0,
        successRate: 0.9, // é»˜è®¤90%æˆåŠŸç‡
        averageLatency: 500,
        p99Latency: 1000,
        averageCost: 0.01,
        lastUsed: new Date()
      };
    }

    private async calculateAvailabilityScore(
      adapter: IModelAdapter,
      stats: AdapterPerformanceStats
    ): Promise<number> {
      // æ£€æŸ¥å¥åº·çŠ¶æ€
      const isHealthy = await adapter.isAvailable();
      if (!isHealthy) return 0;

      // åŸºäºå†å²æˆåŠŸç‡
      return stats.successRate;
    }

    private calculateCostScore(
      info: ModelInfo,
      metrics: any,
      constraints?: { maxCost?: number }
    ): number {
      const costPerToken = info.pricing?.inputPrice || 0;

      // å¦‚æœæœ‰æˆæœ¬é™åˆ¶
      if (constraints?.maxCost) {
        if (costPerToken > constraints.maxCost) {
          return 0; // è¶…å‡ºé¢„ç®—
        }
      }

      // æˆæœ¬è¶Šä½åˆ†æ•°è¶Šé«˜ (å½’ä¸€åŒ–åˆ° 0-1)
      // å‡è®¾æœ€é«˜å¯æ¥å—æˆæœ¬æ˜¯ $0.01 per token
      const maxAcceptableCost = 0.01;
      return Math.max(0, 1 - (costPerToken / maxAcceptableCost));
    }

    private calculatePerformanceScore(
      stats: AdapterPerformanceStats,
      constraints?: { maxLatency?: number }
    ): number {
      // å¦‚æœæœ‰å»¶è¿Ÿé™åˆ¶
      if (constraints?.maxLatency) {
        if (stats.averageLatency > constraints.maxLatency) {
          return 0; // è¶…å‡ºå»¶è¿Ÿé™åˆ¶
        }
      }

      // å»¶è¿Ÿè¶Šä½åˆ†æ•°è¶Šé«˜ (å½’ä¸€åŒ–åˆ° 0-1)
      // å‡è®¾æœ€é«˜å¯æ¥å—å»¶è¿Ÿæ˜¯ 5000ms
      const maxAcceptableLatency = 5000;
      return Math.max(0, 1 - (stats.averageLatency / maxAcceptableLatency));
    }

    private calculatePreferenceScore(
      info: ModelInfo,
      preferences?: { provider?: ModelProvider; model?: string }
    ): number {
      if (!preferences) return 0.5; // ä¸­æ€§åˆ†æ•°

      let score = 0;

      // æä¾›å•†åŒ¹é…
      if (preferences.provider === info.provider) {
        score += 0.5;
      }

      // æ¨¡å‹åŒ¹é…
      if (preferences.model === info.name) {
        score += 0.5;
      }

      return score;
    }
  }

  /**
   * æˆæœ¬ä¼˜å…ˆè·¯ç”±ç­–ç•¥
   *
   * ç›®æ ‡ï¼šåœ¨æ»¡è¶³åŸºæœ¬çº¦æŸçš„å‰æä¸‹ï¼Œé€‰æ‹©æˆæœ¬æœ€ä½çš„æ¨¡å‹
   */
  export class CostOptimizedRoutingStrategy implements IRoutingStrategy {
    name = 'cost_optimized';

    async score(adapter: IModelAdapter, request: RoutingRequest): Promise<number> {
      const info = adapter.getModelInfo();
      const costPerToken = info.pricing?.inputPrice || Infinity;

      // æˆæœ¬è¶Šä½åˆ†æ•°è¶Šé«˜
      // ä½¿ç”¨æŒ‡æ•°å‡½æ•°å¢å¼ºå·®å¼‚
      return Math.exp(-costPerToken * 1000);
    }

    async filter(
      adapter: IModelAdapter,
      request: RoutingRequest,
      context: RoutingContext
    ): Promise<boolean> {
      // å¤ç”¨é»˜è®¤ç­–ç•¥çš„è¿‡æ»¤é€»è¾‘
      const defaultStrategy = new DefaultRoutingStrategy();
      return defaultStrategy.filter(adapter, request, context);
    }
  }

  /**
   * å»¶è¿Ÿä¼˜å…ˆè·¯ç”±ç­–ç•¥
   *
   * ç›®æ ‡ï¼šåœ¨æ»¡è¶³åŸºæœ¬çº¦æŸçš„å‰æä¸‹ï¼Œé€‰æ‹©å»¶è¿Ÿæœ€ä½çš„æ¨¡å‹
   */
  export class LatencyOptimizedRoutingStrategy implements IRoutingStrategy {
    name = 'latency_optimized';

    async score(adapter: IModelAdapter, _request: RoutingRequest, context: RoutingContext): Promise<number> {
      const info = adapter.getModelInfo();
      const stats = context.historicalPerformance.get(info.id);

      if (!stats) {
        return 0.5; // æ²¡æœ‰å†å²æ•°æ®ï¼Œç»™ä¸­æ€§åˆ†æ•°
      }

      // å»¶è¿Ÿè¶Šä½åˆ†æ•°è¶Šé«˜
      // ä½¿ç”¨æŒ‡æ•°å‡½æ•°å¢å¼ºå·®å¼‚
      return Math.exp(-stats.averageLatency / 1000);
    }

    async filter(
      adapter: IModelAdapter,
      request: RoutingRequest,
      context: RoutingContext
    ): Promise<boolean> {
      const defaultStrategy = new DefaultRoutingStrategy();
      return defaultStrategy.filter(adapter, request, context);
    }
  }

  /**
   * æ™ºèƒ½æ¨¡å‹è·¯ç”±å™¨
   *
   * è®¾è®¡ç†å¿µï¼š
   * 1. æ³¨å†Œè¡¨æ¨¡å¼ï¼šç®¡ç†æ‰€æœ‰å¯ç”¨çš„æ¨¡å‹é€‚é…å™¨
   * 2. ç­–ç•¥æ¨¡å¼ï¼šæ”¯æŒå¤šç§è·¯ç”±ç­–ç•¥ï¼Œå¯åŠ¨æ€åˆ‡æ¢
   * 3. è´£ä»»é“¾æ¨¡å¼ï¼šé€‚é…å™¨æŒ‰ä¼˜å…ˆçº§æ’é˜Ÿå¤„ç†
   * 4. ç¼“å­˜ä¼˜åŒ–ï¼šç¼“å­˜è·¯ç”±å†³ç­–ï¼Œå‡å°‘é‡å¤è®¡ç®—
   * 5. å®æ—¶ç›‘æ§ï¼šè·Ÿè¸ªè·¯ç”±å†³ç­–æ•ˆæœ
   */
  export class ModelRouter {
    private adapters: Map<string, IModelAdapter> = new Map();
    private strategies: Map<string, IRoutingStrategy> = new Map();
    private currentStrategy: IRoutingStrategy;
    private decisionCache: Map<string, RoutingDecision> = new Map();
    private performanceStats: Map<string, AdapterPerformanceStats> = new Map();
    private cacheTTL: number = 60000; // 1åˆ†é’Ÿç¼“å­˜

    constructor() {
      // æ³¨å†Œé»˜è®¤ç­–ç•¥
      this.registerStrategy(new DefaultRoutingStrategy());
      this.registerStrategy(new CostOptimizedRoutingStrategy());
      this.registerStrategy(new LatencyOptimizedRoutingStrategy());

      // è®¾ç½®é»˜è®¤ç­–ç•¥
      this.currentStrategy = this.strategies.get('default_balanced')!;

      logger.info('æ¨¡å‹è·¯ç”±å™¨åˆå§‹åŒ–', 'ModelRouter', {
        strategiesCount: this.strategies.size,
        currentStrategy: this.currentStrategy.name
      });
    }

    // ================= é€‚é…å™¨æ³¨å†Œä¸ç®¡ç† =================

    /**
     * æ³¨å†Œé€‚é…å™¨
     */
    registerAdapter(adapter: IModelAdapter): void {
      const info = adapter.getModelInfo();
      const key = `${info.provider}-${info.name}`;

      this.adapters.set(key, adapter);
      this.performanceStats.set(info.id, this.createDefaultStats());

      logger.info('æ¨¡å‹é€‚é…å™¨å·²æ³¨å†Œ', 'ModelRouter', {
        key,
        provider: info.provider,
        model: info.name,
        capabilities: info.capabilities
      });

      metrics.increment('model_router.adapters_registered');
    }

    /**
     * å–æ¶ˆæ³¨å†Œé€‚é…å™¨
     */
    unregisterAdapter(key: string): void {
      const adapter = this.adapters.get(key);
      if (adapter) {
        const info = adapter.getModelInfo();
        this.adapters.delete(key);
        this.performanceStats.delete(info.id);

        logger.info('æ¨¡å‹é€‚é…å™¨å·²å–æ¶ˆæ³¨å†Œ', 'ModelRouter', { key });

        // æ¸…é™¤ç›¸å…³ç¼“å­˜
        this.invalidateCache();
      }
    }

    /**
     * è·å–é€‚é…å™¨
     */
    getAdapter(key: string): IModelAdapter | undefined {
      return this.adapters.get(key);
    }

    /**
     * è·å–æ‰€æœ‰é€‚é…å™¨
     */
    getAllAdapters(): IModelAdapter[] {
      return Array.from(this.adapters.values());
    }

    // ================= ç­–ç•¥ç®¡ç† =================

    /**
     * æ³¨å†Œè·¯ç”±ç­–ç•¥
     */
    registerStrategy(strategy: IRoutingStrategy): void {
      this.strategies.set(strategy.name, strategy);
      logger.info('è·¯ç”±ç­–ç•¥å·²æ³¨å†Œ', 'ModelRouter', {
        strategyName: strategy.name
      });
    }

    /**
     * è®¾ç½®å½“å‰ç­–ç•¥
     */
    setStrategy(strategyName: string): void {
      const strategy = this.strategies.get(strategyName);
      if (!strategy) {
        throw new Error(`ç­–ç•¥ä¸å­˜åœ¨: ${strategyName}`);
      }

      this.currentStrategy = strategy;
      logger.info('è·¯ç”±ç­–ç•¥å·²æ›´æ–°', 'ModelRouter', {
        oldStrategy: this.currentStrategy.name,
        newStrategy: strategy.name
      });

      // ç­–ç•¥æ”¹å˜æ—¶æ¸…é™¤ç¼“å­˜
      this.invalidateCache();
    }

    /**
     * è·å–å½“å‰ç­–ç•¥
     */
    getCurrentStrategy(): IRoutingStrategy {
      return this.currentStrategy;
    }

    // ================= æ ¸å¿ƒè·¯ç”±é€»è¾‘ =================

    /**
     * è·¯ç”±è¯·æ±‚åˆ°æœ€ä½³é€‚é…å™¨
     */
    async route(request: RoutingRequest): Promise<RoutingDecision> {
      const startTime = Date.now();

      logger.debug('å¼€å§‹è·¯ç”±è¯·æ±‚', 'ModelRouter', {
        requestId: this.generateRequestId(),
        strategy: this.currentStrategy.name
      });

      try {
        // 1. æ£€æŸ¥ç¼“å­˜
        const cached = this.checkCache(request);
        if (cached) {
          logger.debug('ä½¿ç”¨ç¼“å­˜çš„è·¯ç”±å†³ç­–', 'ModelRouter');
          return cached;
        }

        // 2. å‡†å¤‡è·¯ç”±ä¸Šä¸‹æ–‡
        const context: RoutingContext = {
          timestamp: new Date(),
          availableAdapters: this.adapters,
          historicalPerformance: this.performanceStats
        };

        // 3. ç­›é€‰é€‚é…å™¨ï¼ˆè¿‡æ»¤ä¸æ»¡è¶³æ¡ä»¶çš„ï¼‰
        const candidateAdapters: Array<{ adapter: IModelAdapter; key: string }> = [];

        for (const [key, adapter] of this.adapters) {
          const isCandidate = await this.currentStrategy.filter(adapter, request, context);
          if (isCandidate) {
            candidateAdapters.push({ adapter, key });
          }
        }

        if (candidateAdapters.length === 0) {
          throw new Error('æ²¡æœ‰æ‰¾åˆ°æ»¡è¶³æ¡ä»¶çš„é€‚é…å™¨');
        }

        logger.debug('å€™é€‰é€‚é…å™¨æ•°é‡', 'ModelRouter', {
          count: candidateAdapters.length,
          totalAdapters: this.adapters.size
        });

        // 4. è¯„åˆ†é€‚é…å™¨
        const scoredAdapters = await Promise.all(
          candidateAdapters.map(async ({ adapter, key }) => ({
            adapter,
            key,
            score: await this.currentStrategy.score(adapter, request, context)
          }))
        );

        // 5. æ’åºå¹¶é€‰æ‹©æœ€ä½³
        scoredAdapters.sort((a, b) => b.score - a.score);
        const best = scoredAdapters[0];
        const bestAdapter = best.adapter;
        const bestInfo = bestAdapter.getModelInfo();

        logger.debug('è·¯ç”±å†³ç­–å®Œæˆ', 'ModelRouter', {
          selectedAdapter: best.key,
          score: best.score,
          candidateCount: scoredAdapters.length
        });

        // 6. æ„å»ºå†³ç­–ç»“æœ
        const decision: RoutingDecision = {
          adapter: bestAdapter,
          model: bestInfo.name,
          reason: this.generateReason(best.score, scoredAdapters),
          score: best.score,
          estimatedLatency: this.performanceStats.get(bestInfo.id)?.averageLatency || 0,
          estimatedCost: bestInfo.pricing?.inputPrice || 0
        };

        // 7. ç¼“å­˜å†³ç­–
        this.cacheDecision(request, decision);

        // 8. æ›´æ–°æŒ‡æ ‡
        const routingTime = Date.now() - startTime;
        metrics.increment('model_router.requests_routed');
        metrics.histogram('model_router.routing_time', routingTime);
        metrics.histogram('model_router.score', best.score);

        return decision;
      } catch (error) {
        logger.error('è·¯ç”±å¤±è´¥', 'ModelRouter', { error });
        throw error;
      }
    }

    /**
     * æ‰¹é‡è·¯ç”±ï¼ˆç”¨äºé¢„çƒ­æˆ–æ€§èƒ½æµ‹è¯•ï¼‰
     */
    async batchRoute(requests: RoutingRequest[]): Promise<RoutingDecision[]> {
      logger.debug('æ‰¹é‡è·¯ç”±è¯·æ±‚', 'ModelRouter', {
        count: requests.length
      });

      // ç®€åŒ–å®ç°ï¼šå¹¶è¡Œè·¯ç”±
      // å®é™…ç”Ÿäº§ä¸­å¯èƒ½éœ€è¦é™æµ
      const decisions = await Promise.all(
        requests.map(request => this.route(request))
      );

      return decisions;
    }

    // ================= æ€§èƒ½ç»Ÿè®¡æ›´æ–° =================

    /**
     * æ›´æ–°é€‚é…å™¨æ€§èƒ½ç»Ÿè®¡
     */
    updatePerformanceStats(
      adapterId: string,
      success: boolean,
      latency: number,
      cost: number
    ): void {
      const stats = this.performanceStats.get(adapterId);
      if (!stats) return;

      stats.totalRequests++;

      // æ»‘åŠ¨çª—å£å¹³å‡ï¼ˆç®€åŒ–å®ç°ï¼‰
      const alpha = 0.1; // å¹³æ»‘å› å­
      stats.averageLatency = alpha * latency + (1 - alpha) * stats.averageLatency;
      stats.averageCost = alpha * cost + (1 - alpha) * stats.averageCost;

      // æˆåŠŸç‡
      stats.successRate = (stats.successRate * (stats.totalRequests - 1) + (success ? 1 : 0)) / stats.totalRequests;

      stats.lastUsed = new Date();

      logger.debug('æ€§èƒ½ç»Ÿè®¡å·²æ›´æ–°', 'ModelRouter', {
        adapterId,
        success,
        latency,
        cost,
        newSuccessRate: stats.successRate
      });
    }

    // ================= ç¼“å­˜ç®¡ç† =================

    /**
     * æ£€æŸ¥ç¼“å­˜
     */
    private checkCache(request: RoutingRequest): RoutingDecision | null {
      const cacheKey = this.generateCacheKey(request);
      const cached = this.decisionCache.get(cacheKey);

      if (!cached) {
        return null;
      }

      // æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
      const age = Date.now() - cached.adapter.getModelInfo().version.length; // ç®€åŒ–
      if (age > this.cacheTTL) {
        this.decisionCache.delete(cacheKey);
        return null;
      }

      return cached;
    }

    /**
     * ç¼“å­˜å†³ç­–
     */
    private cacheDecision(request: RoutingRequest, decision: RoutingDecision): void {
      const cacheKey = this.generateCacheKey(request);
      this.decisionCache.set(cacheKey, decision);

      // é™åˆ¶ç¼“å­˜å¤§å°
      if (this.decisionCache.size > 1000) {
        // ç®€å•çš„LRUï¼šåˆ é™¤æœ€æ—§çš„ï¼ˆå®é™…åº”ä½¿ç”¨LRUç®—æ³•ï¼‰
        const firstKey = this.decisionCache.keys().next().value;
        this.decisionCache.delete(firstKey);
      }
    }

    /**
     * ä½¿ç¼“å­˜å¤±æ•ˆ
     */
    private invalidateCache(): void {
      this.decisionCache.clear();
      logger.debug('è·¯ç”±ç¼“å­˜å·²æ¸…é™¤', 'ModelRouter');
    }

    // ================= å·¥å…·æ–¹æ³• =================

    private generateRequestId(): string {
      return `req-${Date.now()}-${Math.random().toString(36).substring(2, 15)}`;
    }

    private generateCacheKey(request: RoutingRequest): string {
      // ç®€åŒ–çš„ç¼“å­˜é”®ç”Ÿæˆ
      const constraints = request.constraints || {};
      const preferences = request.preferences || {};
      return JSON.stringify({
        type: request.request.type,
        constraints,
        preferences
      });
    }

    private generateReason(score: number, candidates: Array<{ score: number }>): string {
      const percentage = (score * 100).toFixed(2);
      return `Selected with ${percentage}% score out of ${candidates.length} candidates`;
    }

    private createDefaultStats(): AdapterPerformanceStats {
      return {
        totalRequests: 0,
        successRate: 1.0,
        averageLatency: 0,
        p99Latency: 0,
        averageCost: 0,
        lastUsed: new Date()
      };
    }
  }

  // ============ å¯¼å‡º ============

  export default ModelRouter;

### 3.8 é€‚é…å™¨å·¥å‚ä¸æ³¨å†Œè¡¨å®ç°

  // ğŸ“¦ src/model/AdapterFactory.ts - é€‚é…å™¨å·¥å‚å®ç°
  import { logger } from '../utils/logger';
  import { metrics } from '../utils/metrics';
  import { IModelAdapter } from './BaseModelAdapter';
  import { OpenAIAdapter, OpenAIAdapterConfig } from './OpenAIAdapter';
  import { LocalModelAdapter, LocalModelAdapterConfig } from './LocalModelAdapter';
  import {
    ModelAdapterConfig,
    ModelProvider,
    ModelInfo
  } from '../types/model.types';

  /**
   * é€‚é…å™¨åˆ›å»ºå™¨å‡½æ•°ç±»å‹
   */
  type AdapterCreator = (
    config: ModelAdapterConfig,
    modelInfo: ModelInfo
  ) => IModelAdapter;

  /**
   * é€‚é…å™¨å…ƒä¿¡æ¯
   */
  interface AdapterMetadata {
    provider: ModelProvider;
    version: string;
    creator: AdapterCreator;
    defaultConfig?: Partial<ModelAdapterConfig>;
    supportedModels: string[];
  }

  /**
   * é€‚é…å™¨å·¥å‚
   *
   * è®¾è®¡ç†å¿µï¼š
   * 1. å·¥å‚æ–¹æ³•æ¨¡å¼ï¼šå°è£…å¯¹è±¡åˆ›å»ºé€»è¾‘
   * 2. æ³¨å†Œè¡¨æ¨¡å¼ï¼šåŠ¨æ€æ³¨å†Œå’Œç®¡ç†é€‚é…å™¨ç±»å‹
   * 3. å•ä¾‹æ¨¡å¼ï¼šå¯¹äºæ˜‚è´µçš„é€‚é…å™¨ï¼Œä½¿ç”¨å•ä¾‹æ¨¡å¼å¤ç”¨
   * 4. ä¾èµ–æ³¨å…¥ï¼šé€šè¿‡é…ç½®æ³¨å…¥ä¾èµ–
   * 5. å»¶è¿Ÿåˆå§‹åŒ–ï¼šé€‚é…å™¨åœ¨ç¬¬ä¸€æ¬¡ä½¿ç”¨æ—¶æ‰åˆ›å»º
   */
  export class AdapterFactory {
    private registry: Map<string, AdapterMetadata> = new Map();
    private instances: Map<string, IModelAdapter> = new Map();
    private modelInfoRegistry: Map<string, ModelInfo> = new Map();

    constructor() {
      // æ³¨å†Œå†…ç½®é€‚é…å™¨
      this.registerBuiltinAdapters();

      logger.info('é€‚é…å™¨å·¥å‚åˆå§‹åŒ–', 'AdapterFactory', {
        registeredAdapters: Array.from(this.registry.keys())
      });
    }

    // ================= æ³¨å†Œç®¡ç† =================

    /**
     * æ³¨å†Œé€‚é…å™¨ç±»å‹
     */
    registerAdapter(
      key: string,
      provider: ModelProvider,
      creator: AdapterCreator,
      metadata?: Partial<AdapterMetadata>
    ): void {
      const adapterMetadata: AdapterMetadata = {
        provider,
        version: metadata?.version || '1.0.0',
        creator,
        defaultConfig: metadata?.defaultConfig,
        supportedModels: metadata?.supportedModels || []
      };

      this.registry.set(key, adapterMetadata);

      logger.info('é€‚é…å™¨ç±»å‹å·²æ³¨å†Œ', 'AdapterFactory', {
        key,
        provider,
        version: adapterMetadata.version,
        supportedModels: adapterMetadata.supportedModels.length
      });

      metrics.increment('adapter_factory.types_registered');
    }

    /**
     * æ³¨é”€é€‚é…å™¨ç±»å‹
     */
    unregisterAdapter(key: string): void {
      this.registry.delete(key);
      logger.info('é€‚é…å™¨ç±»å‹å·²æ³¨é”€', 'AdapterFactory', { key });
    }

    /**
     * æ³¨å†Œæ¨¡å‹ä¿¡æ¯
     */
    registerModelInfo(modelInfo: ModelInfo): void {
      const key = `${modelInfo.provider}-${modelInfo.id}`;
      this.modelInfoRegistry.set(key, modelInfo);

      logger.debug('æ¨¡å‹ä¿¡æ¯å·²æ³¨å†Œ', 'AdapterFactory', {
        key,
        modelId: modelInfo.id,
        capabilities: modelInfo.capabilities
      });
    }

    // ================= å®ä¾‹åˆ›å»º =================

    /**
     * åˆ›å»ºé€‚é…å™¨å®ä¾‹
     */
    async createAdapter(
      config: ModelAdapterConfig,
      useSingleton: boolean = false
    ): Promise<IModelAdapter> {
      const key = this.generateInstanceKey(config);

      logger.debug('åˆ›å»ºé€‚é…å™¨å®ä¾‹', 'AdapterFactory', {
        key,
        useSingleton,
        provider: config.provider,
        model: config.model
      });

      try {
        // 1. æ£€æŸ¥å•ä¾‹ç¼“å­˜
        if (useSingleton) {
          const cachedInstance = this.instances.get(key);
          if (cachedInstance) {
            logger.debug('ä½¿ç”¨å•ä¾‹å®ä¾‹', 'AdapterFactory', { key });
            return cachedInstance;
          }
        }

        // 2. æŸ¥æ‰¾é€‚é…å™¨å…ƒä¿¡æ¯
        const metadata = this.findAdapterMetadata(config);
        if (!metadata) {
          throw new Error(`æœªæ‰¾åˆ°é€‚é…å™¨ç±»å‹: ${config.provider}`);
        }

        // 3. è·å–æ¨¡å‹ä¿¡æ¯
        const modelInfo = await this.getOrCreateModelInfo(config);

        // 4. åˆå¹¶é…ç½®
        const finalConfig = {
          ...metadata.defaultConfig,
          ...config
        };

        // 5. åˆ›å»ºé€‚é…å™¨å®ä¾‹
        const adapter = metadata.creator(finalConfig, modelInfo);

        // 6. åˆå§‹åŒ–é€‚é…å™¨
        await adapter.initialize();

        // 7. ç¼“å­˜å•ä¾‹å®ä¾‹
        if (useSingleton) {
          this.instances.set(key, adapter);
          logger.info('é€‚é…å™¨å®ä¾‹å·²ç¼“å­˜ä¸ºå•ä¾‹', 'AdapterFactory', { key });
        }

        // 8. æ›´æ–°æŒ‡æ ‡
        metrics.increment('adapter_factory.instances_created');

        return adapter;
      } catch (error) {
        logger.error('åˆ›å»ºé€‚é…å™¨å®ä¾‹å¤±è´¥', 'AdapterFactory', {
          key,
          error
        });

        metrics.increment('adapter_factory.creation_failed');

        throw error;
      }
    }

    /**
     * è·å–æˆ–åˆ›å»ºé€‚é…å™¨å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰
     */
    async getOrCreateAdapter(config: ModelAdapterConfig): Promise<IModelAdapter> {
      return this.createAdapter(config, true);
    }

    // ================= æ‰¹é‡æ“ä½œ =================

    /**
     * æ‰¹é‡åˆ›å»ºé€‚é…å™¨
     */
    async createAdapters(
      configs: ModelAdapterConfig[],
      useSingleton: boolean = false
    ): Promise<IModelAdapter[]> {
      logger.info('æ‰¹é‡åˆ›å»ºé€‚é…å™¨', 'AdapterFactory', {
        count: configs.length,
        useSingleton
      });

      const adapters = await Promise.all(
        configs.map(config => this.createAdapter(config, useSingleton))
      );

      return adapters;
    }

    /**
     * åˆ›å»ºæ‰€æœ‰å·²æ³¨å†Œé€‚é…å™¨çš„å®ä¾‹
     */
    async createAllRegisteredAdapters(
      configs: ModelAdapterConfig[],
      useSingleton: boolean = false
    ): Promise<Map<string, IModelAdapter>> {
      logger.info('åˆ›å»ºæ‰€æœ‰å·²æ³¨å†Œé€‚é…å™¨çš„å®ä¾‹', 'AdapterFactory', {
        count: configs.length,
        useSingleton
      });

      const instances = new Map<string, IModelAdapter>();

      for (const config of configs) {
        try {
          const adapter = await this.createAdapter(config, useSingleton);
          const key = this.generateInstanceKey(config);
          instances.set(key, adapter);
        } catch (error) {
          logger.warn('åˆ›å»ºé€‚é…å™¨å®ä¾‹å¤±è´¥ï¼Œè·³è¿‡', 'AdapterFactory', {
            provider: config.provider,
            model: config.model,
            error
          });
        }
      }

      logger.info('é€‚é…å™¨å®ä¾‹åˆ›å»ºå®Œæˆ', 'AdapterFactory', {
        successCount: instances.size,
        totalCount: configs.length
      });

      return instances;
    }

    // ================= å®ä¾‹ç®¡ç† =================

    /**
     * è·å–ç¼“å­˜çš„å®ä¾‹
     */
    getCachedInstance(key: string): IModelAdapter | undefined {
      return this.instances.get(key);
    }

    /**
     * é”€æ¯å®ä¾‹
     */
    async destroyInstance(key: string): Promise<void> {
      const instance = this.instances.get(key);
      if (instance) {
        // è°ƒç”¨å®ä¾‹çš„æ¸…ç†æ–¹æ³•ï¼ˆå¦‚æœæœ‰ï¼‰
        // è¿™é‡Œç®€åŒ–å¤„ç†
        this.instances.delete(key);
        logger.info('é€‚é…å™¨å®ä¾‹å·²é”€æ¯', 'AdapterFactory', { key });
      }
    }

    /**
     * æ¸…ç©ºæ‰€æœ‰å®ä¾‹
     */
    async clearAllInstances(): Promise<void> {
      logger.info('æ¸…ç©ºæ‰€æœ‰é€‚é…å™¨å®ä¾‹', 'AdapterFactory', {
        count: this.instances.size
      });

      // æ¸…ç†æ¯ä¸ªå®ä¾‹
      for (const [key, instance] of this.instances) {
        try {
          // è°ƒç”¨å®ä¾‹çš„æ¸…ç†æ–¹æ³•ï¼ˆå¦‚æœæœ‰ï¼‰
          // è¿™é‡Œç®€åŒ–å¤„ç†
          logger.debug('æ¸…ç†å®ä¾‹', 'AdapterFactory', { key });
        } catch (error) {
          logger.warn('æ¸…ç†å®ä¾‹å¤±è´¥', 'AdapterFactory', {
            key,
            error
          });
        }
      }

      this.instances.clear();
    }

    // ================= ç§æœ‰è¾…åŠ©æ–¹æ³• =================

    /**
     * æ³¨å†Œå†…ç½®é€‚é…å™¨
     */
    private registerBuiltinAdapters(): void {
      // OpenAI é€‚é…å™¨
      this.registerAdapter(
        'openai',
        ModelProvider.OPENAI,
        (config: ModelAdapterConfig, modelInfo: ModelInfo) => {
          return new OpenAIAdapter(config as OpenAIAdapterConfig, modelInfo);
        },
        {
          version: '1.0.0',
          defaultConfig: {
            timeout: 30000,
            retryPolicy: {
              maxRetries: 3,
              backoffFactor: 2,
              initialDelay: 1000
            },
            cache: {
              enabled: true,
              ttl: 300000,
              maxSize: 1000
            },
            logging: {
              enabled: true,
              level: 'info'
            }
          },
          supportedModels: [
            'gpt-4',
            'gpt-4-turbo-preview',
            'gpt-3.5-turbo',
            'text-embedding-ada-002',
            'text-embedding-3-small',
            'text-embedding-3-large'
          ]
        }
      );

      // æœ¬åœ°æ¨¡å‹é€‚é…å™¨
      this.registerAdapter(
        'local',
        ModelProvider.LOCAL,
        (config: ModelAdapterConfig, modelInfo: ModelInfo) => {
          return new LocalModelAdapter(config as LocalModelAdapterConfig, modelInfo);
        },
        {
          version: '1.0.0',
          defaultConfig: {
            timeout: 60000,
            retryPolicy: {
              maxRetries: 1,
              backoffFactor: 2,
              initialDelay: 1000
            },
            logging: {
              enabled: true,
              level: 'debug'
            }
          },
          supportedModels: [
            'llama-2-7b',
            'llama-2-13b',
            'llama-2-70b',
            'mistral-7b',
            'codellama-7b'
          ]
        }
      );
    }

    /**
     * æŸ¥æ‰¾é€‚é…å™¨å…ƒä¿¡æ¯
     */
    private findAdapterMetadata(config: ModelAdapterConfig): AdapterMetadata | undefined {
      // ç›´æ¥é€šè¿‡keyæŸ¥æ‰¾
      const key = config.provider.toString();
      let metadata = this.registry.get(key);

      // å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå°è¯•é€šè¿‡provideræŸ¥æ‰¾
      if (!metadata) {
        for (const [_, value] of this.registry) {
          if (value.provider === config.provider) {
            metadata = value;
            break;
          }
        }
      }

      return metadata;
    }

    /**
     * è·å–æˆ–åˆ›å»ºæ¨¡å‹ä¿¡æ¯
     */
    private async getOrCreateModelInfo(config: ModelAdapterConfig): Promise<ModelInfo> {
      const key = `${config.provider}-${config.model}`;
      let modelInfo = this.modelInfoRegistry.get(key);

      if (!modelInfo) {
        // åˆ›å»ºé»˜è®¤æ¨¡å‹ä¿¡æ¯
        // å®é™…é¡¹ç›®ä¸­ï¼Œè¿™é‡Œåº”è¯¥è°ƒç”¨æ¨¡å‹APIè·å–è¯¦ç»†ä¿¡æ¯
        modelInfo = {
          id: config.model,
          name: config.model,
          provider: config.provider,
          type: 'chat_llm' as any,
          capabilities: [/* é»˜è®¤èƒ½åŠ› */],
          pricing: {
            inputPrice: 0.01,
            outputPrice: 0.02,
            currency: 'USD'
          },
          limits: {
            maxInputTokens: 4096,
            maxOutputTokens: 4096,
            maxRequestsPerMinute: 3000,
            maxTokensPerMinute: 100000
          },
          version: '1.0.0',
          status: 'available'
        };

        this.registerModelInfo(modelInfo);
      }

      return modelInfo;
    }

    /**
     * ç”Ÿæˆå®ä¾‹é”®
     */
    private generateInstanceKey(config: ModelAdapterConfig): string {
      return `${config.provider}-${config.model}-${config.timeout || 0}`;
    }
  }

  // ============ å¯¼å‡º ============

  export default AdapterFactory;

### 3.9 é›†æˆç¤ºä¾‹ï¼šå®Œæ•´çš„æ¨¡å‹ç®¡ç†å™¨

  // ğŸ“¦ src/model/ModelManager.ts - å®Œæ•´çš„æ¨¡å‹ç®¡ç†å™¨é›†æˆç¤ºä¾‹
  import { logger } from '../utils/logger';
  import { metrics } from '../utils/metrics';
  import { AdapterFactory } from './AdapterFactory';
  import { ModelRouter, IRoutingStrategy } from './ModelRouter';
  import { IModelAdapter } from './BaseModelAdapter';
  import {
    ModelAdapterConfig,
    CompletionRequest,
    CompletionResponse,
    ChatRequest,
    ChatResponse,
    RoutingRequest,
    ModelProvider
  } from '../types/model.types';

  /**
   * æ¨¡å‹ç®¡ç†å™¨
   *
   * è®¾è®¡ç†å¿µï¼š
   * è¿™æ˜¯ä¸€ä¸ªé—¨é¢æ¨¡å¼çš„å®ç°ï¼Œå°†å·¥å‚ã€è·¯ç”±å™¨å’Œé€‚é…å™¨æ•´åˆåœ¨ä¸€èµ·ï¼Œ
   * å¯¹å¤–æä¾›ä¸€ä¸ªç»Ÿä¸€ã€ç®€æ´çš„æ¥å£ã€‚
   *
   * åŠŸèƒ½ï¼š
   * 1. ç®¡ç†å¤šä¸ªæ¨¡å‹é€‚é…å™¨çš„ç”Ÿå‘½å‘¨æœŸ
   * 2. æä¾›æ™ºèƒ½è·¯ç”±ï¼Œè‡ªåŠ¨é€‰æ‹©æœ€ä½³æ¨¡å‹
   * 3. å°è£…å¤æ‚çš„è°ƒç”¨ç»†èŠ‚
   * 4. æä¾›ç»Ÿä¸€çš„ç›‘æ§å’Œæ—¥å¿—
   */
  export class ModelManager {
    private factory: AdapterFactory;
    private router: ModelRouter;
    private activeAdapters: Map<string, IModelAdapter> = new Map();

    constructor() {
      this.factory = new AdapterFactory();
      this.router = new ModelRouter();

      logger.info('æ¨¡å‹ç®¡ç†å™¨åˆå§‹åŒ–', 'ModelManager');
    }

    // ================= ç”Ÿå‘½å‘¨æœŸç®¡ç† =================

    /**
     * åˆå§‹åŒ–æ¨¡å‹ç®¡ç†å™¨
     */
    async initialize(configs: ModelAdapterConfig[]): Promise<void> {
      logger.info('åˆå§‹åŒ–æ¨¡å‹ç®¡ç†å™¨', 'ModelManager', {
        configCount: configs.length
      });

      try {
        // 1. åˆ›å»ºæ‰€æœ‰é€‚é…å™¨
        const instances = await this.factory.createAllRegisteredAdapters(configs, true);

        // 2. æ³¨å†Œåˆ°è·¯ç”±å™¨
        for (const [key, adapter] of instances) {
          this.activeAdapters.set(key, adapter);
          this.router.registerAdapter(adapter);
        }

        logger.info('æ¨¡å‹ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ', 'ModelManager', {
          adapterCount: instances.size
        });
      } catch (error) {
        logger.error('æ¨¡å‹ç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥', 'ModelManager', { error });
        throw error;
      }
    }

    /**
     * å…³é—­æ¨¡å‹ç®¡ç†å™¨
     */
    async shutdown(): Promise<void> {
      logger.info('å…³é—­æ¨¡å‹ç®¡ç†å™¨', 'ModelManager');

      try {
        // æ¸…ç†æ‰€æœ‰é€‚é…å™¨å®ä¾‹
        await this.factory.clearAllInstances();

        this.activeAdapters.clear();

        logger.info('æ¨¡å‹ç®¡ç†å™¨å·²å…³é—­', 'ModelManager');
      } catch (error) {
        logger.error('å…³é—­æ¨¡å‹ç®¡ç†å™¨å¤±è´¥', 'ModelManager', { error });
        throw error;
      }
    }

    // ================= é«˜çº§æ¥å£ï¼ˆè‡ªåŠ¨è·¯ç”±ï¼‰=================

    /**
     * æ™ºèƒ½ç”Ÿæˆè¡¥å…¨ï¼ˆè‡ªåŠ¨è·¯ç”±ï¼‰
     */
    async generateCompletion(request: CompletionRequest): Promise<CompletionResponse> {
      const startTime = Date.now();

      logger.debug('æ™ºèƒ½ç”Ÿæˆè¡¥å…¨', 'ModelManager', {
        prompt: request.prompt.substring(0, 100) + '...'
      });

      try {
        // 1. æ„å»ºè·¯ç”±è¯·æ±‚
        const routingRequest: RoutingRequest = {
          request,
          constraints: {
            maxLatency: 5000,  // 5ç§’
            maxCost: 0.05      // $0.05
          },
          preferences: {
            provider: request.provider
          }
        };

        // 2. è·¯ç”±åˆ°æœ€ä½³é€‚é…å™¨
        const decision = await this.router.route(routingRequest);

        logger.info('è·¯ç”±å†³ç­–', 'ModelManager', {
          selectedModel: decision.model,
          score: decision.score,
          reason: decision.reason
        });

        // 3. è°ƒç”¨é€‚é…å™¨
        const response = await decision.adapter.generateCompletion(request);

        // 4. æ›´æ–°æ€§èƒ½ç»Ÿè®¡
        this.router.updatePerformanceStats(
          decision.adapter.getModelInfo().id,
          response.success,
          response.metadata?.processingTime || 0,
          this.calculateCost(response.usage.totalTokens, decision)
        );

        return response;
      } catch (error) {
        logger.error('æ™ºèƒ½ç”Ÿæˆè¡¥å…¨å¤±è´¥', 'ModelManager', { error });
        throw error;
      }
    }

    /**
     * æ™ºèƒ½ç”ŸæˆèŠå¤©è¡¥å…¨ï¼ˆè‡ªåŠ¨è·¯ç”±ï¼‰
     */
    async generateChatCompletion(request: ChatRequest): Promise<ChatResponse> {
      const startTime = Date.now();

      logger.debug('æ™ºèƒ½ç”ŸæˆèŠå¤©è¡¥å…¨', 'ModelManager', {
        messageCount: request.messages.length
      });

      try {
        // 1. æ„å»ºè·¯ç”±è¯·æ±‚
        const routingRequest: RoutingRequest = {
          request,
          constraints: {
            maxLatency: 5000,
            maxCost: 0.05
          },
          preferences: {
            provider: request.provider
          }
        };

        // 2. è·¯ç”±åˆ°æœ€ä½³é€‚é…å™¨
        const decision = await this.router.route(routingRequest);

        logger.info('è·¯ç”±å†³ç­–', 'ModelManager', {
          selectedModel: decision.model,
          score: decision.score,
          reason: decision.reason
        });

        // 3. è°ƒç”¨é€‚é…å™¨
        const response = await decision.adapter.generateChatCompletion(request);

        // 4. æ›´æ–°æ€§èƒ½ç»Ÿè®¡
        this.router.updatePerformanceStats(
          decision.adapter.getModelInfo().id,
          response.success,
          response.metadata?.processingTime || 0,
          this.calculateCost(response.usage.totalTokens, decision)
        );

        return response;
      } catch (error) {
        logger.error('æ™ºèƒ½ç”ŸæˆèŠå¤©è¡¥å…¨å¤±è´¥', 'ModelManager', { error });
        throw error;
      }
    }

    // ================= é…ç½®ç®¡ç† =================

    /**
     * è®¾ç½®è·¯ç”±ç­–ç•¥
     */
    setRoutingStrategy(strategyName: string): void {
      this.router.setStrategy(strategyName);
      logger.info('è·¯ç”±ç­–ç•¥å·²æ›´æ–°', 'ModelManager', {
        strategy: strategyName
      });
    }

    /**
     * è·å–è·¯ç”±å™¨
     */
    getRouter(): ModelRouter {
      return this.router;
    }

    /**
     * è·å–å·¥å‚
     */
    getFactory(): AdapterFactory {
      return this.factory;
    }

    /**
     * è·å–æ‰€æœ‰æ´»è·ƒçš„é€‚é…å™¨
     */
    getActiveAdapters(): IModelAdapter[] {
      return Array.from(this.activeAdapters.values());
    }

    // ================= è¾…åŠ©æ–¹æ³• =================

    /**
     * è®¡ç®—æˆæœ¬
     */
    private calculateCost(tokens: number, decision: any): number {
      const modelInfo = decision.adapter.getModelInfo();
      const price = modelInfo.pricing?.inputPrice || 0;
      return (tokens / 1000) * price;
    }
  }

  // ============ ä½¿ç”¨ç¤ºä¾‹ =================

  /**
   * ç¤ºä¾‹ï¼šå¦‚ä½•ä½¿ç”¨æ¨¡å‹ç®¡ç†å™¨
   */
  async function exampleUsage() {
    // 1. åˆ›å»ºæ¨¡å‹ç®¡ç†å™¨
    const manager = new ModelManager();

    // 2. é…ç½®æ¨¡å‹é€‚é…å™¨
    const configs: ModelAdapterConfig[] = [
      {
        provider: ModelProvider.OPENAI,
        model: 'gpt-4-turbo-preview',
        apiKey: process.env.OPENAI_API_KEY!
      },
      {
        provider: ModelProvider.LOCAL,
        model: 'llama-2-7b',
        modelPath: '/models/llama-2-7b.gguf',
        engine: 'llama-cpp'
      }
    ];

    // 3. åˆå§‹åŒ–
    await manager.initialize(configs);

    // 4. ä½¿ç”¨è‡ªåŠ¨è·¯ç”±
    const chatRequest: ChatRequest = {
      provider: ModelProvider.OPENAI, // å¯é€‰ï¼Œè¡¨ç¤ºåå¥½
      model: 'gpt-4-turbo-preview',
      type: 'chat_llm' as any,
      messages: [
        { role: 'user', content: 'Hello, YYCÂ³!' }
      ]
    };

    // æ™ºèƒ½è·¯ç”±ä¼šè‡ªåŠ¨é€‰æ‹©æœ€ä½³æ¨¡å‹ï¼ˆå¯èƒ½æ˜¯OpenAIï¼Œä¹Ÿå¯èƒ½æ˜¯æœ¬åœ°æ¨¡å‹ï¼‰
    const response = await manager.generateChatCompletion(chatRequest);
    console.log('AIå›å¤:', response.message.content);

    // 5. åˆ‡æ¢è·¯ç”±ç­–ç•¥
    manager.setRoutingStrategy('cost_optimized');

    // 6. å†æ¬¡è¯·æ±‚ï¼Œä¼šä¼˜å…ˆé€‰æ‹©æˆæœ¬æ›´ä½çš„æ¨¡å‹
    const costOptimizedResponse = await manager.generateChatCompletion(chatRequest);
    console.log('æˆæœ¬ä¼˜åŒ–å›å¤:', costOptimizedResponse.message.content);

    // 7. å…³é—­
    await manager.shutdown();
  }

  // ============ å¯¼å‡º ============

  export default ModelManager;

--------

## ğŸ¯ ç¬¬ä¸‰ç« æ€»ç»“ä¸å¯¼å¸ˆå¯„è¯­

### 3.10 æœ¬ç« å›é¡¾

æˆ‘ä»¬å®Œæˆäº†ä»¥ä¸‹æ ¸å¿ƒæ¨¡å—ï¼š

1. ç±»å‹å®šä¹‰ ( model.types.ts )
  â€¢ å®šä¹‰äº†å®Œæ•´çš„æ¨¡å‹è¯·æ±‚/å“åº”ç±»å‹
  â€¢ å®šä¹‰äº†è·¯ç”±ã€é€‚é…å™¨ã€ç­–ç•¥ç­‰æ‰€æœ‰æ ¸å¿ƒæ¥å£
  â€¢ ä¸ºæ•´ä¸ªæ¨¡å‹å±‚æä¾›äº†TypeScriptç±»å‹å®‰å…¨
2. æŠ½è±¡åŸºç±» ( BaseModelAdapter.ts )
  â€¢ å®ç°äº†æ¨¡æ¿æ–¹æ³•æ¨¡å¼ï¼Œå®šä¹‰äº†ç»Ÿä¸€çš„è°ƒç”¨æµç¨‹
  â€¢ å°è£…äº†é¢„å¤„ç†ã€åå¤„ç†ã€ç¼“å­˜ã€é‡è¯•ã€ç›‘æ§ç­‰æ¨ªåˆ‡å…³æ³¨ç‚¹
  â€¢ ä¸ºå…·ä½“é€‚é…å™¨æä¾›äº†åšå®çš„åŸºç±»
3. OpenAIé€‚é…å™¨ ( OpenAIAdapter.ts )
  â€¢ å±•ç¤ºäº†å¦‚ä½•å¯¹æ¥ç¬¬ä¸‰æ–¹API
  â€¢ å®ç°äº†APIå¯†é’¥éªŒè¯ã€é€Ÿç‡é™åˆ¶ã€é”™è¯¯å¤„ç†
  â€¢ å®Œå…¨å…¼å®¹OpenAI APIè§„èŒƒ
4. æœ¬åœ°æ¨¡å‹é€‚é…å™¨ ( LocalModelAdapter.ts )
  â€¢ å±•ç¤ºäº†å¦‚ä½•å¤„ç†æœ¬åœ°æ¨¡å‹æ¨ç†
  â€¢ å®ç°äº†æ¨¡å‹åŠ è½½ã€å†…å­˜ç®¡ç†ã€èµ„æºé‡Šæ”¾
  â€¢ æ”¯æŒç¦»çº¿è¿è¡Œï¼Œé›¶APIæˆæœ¬
5. æ™ºèƒ½è·¯ç”±å™¨ ( ModelRouter.ts )
  â€¢ å®ç°äº†ç­–ç•¥æ¨¡å¼ï¼Œæ”¯æŒå¤šç§è·¯ç”±ç­–ç•¥
  â€¢ å®ç°äº†è¯„åˆ†ç³»ç»Ÿï¼Œæ™ºèƒ½é€‰æ‹©æœ€ä½³æ¨¡å‹
  â€¢ é›†æˆäº†æ€§èƒ½ç›‘æ§ï¼Œæ”¯æŒåŠ¨æ€å­¦ä¹ 
6. é€‚é…å™¨å·¥å‚ ( AdapterFactory.ts )
  â€¢ å®ç°äº†å·¥å‚æ–¹æ³•æ¨¡å¼ï¼Œå°è£…å¯¹è±¡åˆ›å»º
  â€¢ å®ç°äº†æ³¨å†Œè¡¨æ¨¡å¼ï¼Œæ”¯æŒåŠ¨æ€æ³¨å†Œé€‚é…å™¨
  â€¢ å®ç°äº†å•ä¾‹æ¨¡å¼ï¼Œä¼˜åŒ–èµ„æºä½¿ç”¨
7. æ¨¡å‹ç®¡ç†å™¨ ( ModelManager.ts )
  â€¢ é—¨é¢æ¨¡å¼ï¼Œæ•´åˆæ‰€æœ‰æ¨¡å—ï¼Œæä¾›ç»Ÿä¸€æ¥å£
  â€¢ è‡ªåŠ¨åŒ–è·¯ç”±ï¼Œæ— éœ€å…³å¿ƒåº•å±‚ç»†èŠ‚
  â€¢ å®Œæ•´çš„ç”Ÿå‘½å‘¨æœŸç®¡ç†


### 3.11 è®¾è®¡æ¨¡å¼æ€»ç»“

 è®¾è®¡æ¨¡å¼                              â”‚ åº”ç”¨ä½ç½®                              â”‚ ä»·å€¼
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 æ¨¡æ¿æ–¹æ³•                              â”‚ BaseModelAdapter                      â”‚ å®šä¹‰ç®—æ³•éª¨æ¶ï¼Œå­ç±»å®ç°ç»†èŠ‚
 ç­–ç•¥æ¨¡å¼                              â”‚ ModelRouter                           â”‚ è·¯ç”±ç®—æ³•å¯åŠ¨æ€åˆ‡æ¢
 å·¥å‚æ–¹æ³•                              â”‚ AdapterFactory                        â”‚ å°è£…å¯¹è±¡åˆ›å»ºé€»è¾‘
 æ³¨å†Œè¡¨æ¨¡å¼                            â”‚ AdapterFactory                        â”‚ åŠ¨æ€ç®¡ç†é€‚é…å™¨ç±»å‹
 å•ä¾‹æ¨¡å¼                              â”‚ AdapterFactory                        â”‚ å¤ç”¨æ˜‚è´µèµ„æº
 é—¨é¢æ¨¡å¼                              â”‚ ModelManager                          â”‚ ç®€åŒ–å¤–éƒ¨è°ƒç”¨æ¥å£
 è£…é¥°å™¨æ¨¡å¼                            â”‚ BaseModelAdapter                      â”‚ æ·»åŠ ç¼“å­˜ã€ç›‘æ§ç­‰åŠŸèƒ½

### 3.12 "äº”é«˜äº”æ ‡äº”åŒ–"ç¬¦åˆåº¦æå‡

 ç»´åº¦                      â”‚ è®¾è®¡é˜¶æ®µ                  â”‚ å®ç°é˜¶æ®µï¼ˆç¬¬ä¸‰ç« ï¼‰        â”‚ è¯´æ˜
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 é«˜å¯ç”¨                    â”‚ â­â­â­â­â­                â”‚ â­â­â­â­                  â”‚ å®ç°äº†å¥åº·æ£€æŸ¥ã€é‡è¯•ã€è·¯ç”±åˆ‡æ¢
 é«˜æ€§èƒ½                    â”‚ â­â­â­â­â­                â”‚ â­â­â­â­                  â”‚ å®ç°äº†ç¼“å­˜ã€å•ä¾‹ã€æ‰¹é‡å¤„ç†
 é«˜å®‰å…¨                    â”‚ â­â­â­â­                  â”‚ â­â­â­â­                  â”‚ å®ç°äº†APIå¯†é’¥ç®¡ç†ã€é€Ÿç‡é™åˆ¶
 é«˜æ‰©å±•                    â”‚ â­â­â­â­â­                â”‚ â­â­â­â­â­                â”‚ æ’ä»¶åŒ–æ¶æ„ï¼Œæ–°å¢æ¨¡å‹æå…¶ç®€å•
 é«˜å¯ç»´æŠ¤                  â”‚ â­â­â­â­â­                â”‚ â­â­â­â­â­                â”‚ æ¸…æ™°çš„å±‚æ¬¡ç»“æ„ï¼Œå®Œæ•´çš„æ—¥å¿—å’Œç›‘æ§

ç¬¬ä¸‰ç« å®ç°å®Œæˆåº¦ï¼šä» 0% æå‡åˆ° 30% ğŸ‰

### 3.13 å¯¼å¸ˆå¯„è¯­ ğŸŒŸ

â”‚ "ç¬¬ä¸‰ç« æ˜¯æ•´ä¸ªç³»ç»Ÿæœ€å¤æ‚çš„éƒ¨åˆ†ä¹‹ä¸€ï¼Œä½†æˆ‘ä»¬æŠŠå®ƒæ‹†è§£å¾—æ¸…æ¸…æ¥šæ¥šã€‚"
â”‚
â”‚ å…³é”®æ”¶è·ï¼š
â”‚
â”‚ 1. æŠ½è±¡çš„åŠ›é‡ï¼šé€šè¿‡æŠ½è±¡åŸºç±»ï¼Œæˆ‘ä»¬å°†å¤æ‚çš„æ¨¡å‹è°ƒç”¨ç»Ÿä¸€æˆäº†ç®€å•çš„æ¥å£
â”‚ 2. ç­–ç•¥çš„çµæ´»æ€§ï¼šé€šè¿‡ç­–ç•¥æ¨¡å¼ï¼Œè·¯ç”±é€»è¾‘å¯ä»¥éšæ—¶è°ƒæ•´ï¼Œæ— éœ€ä¿®æ”¹ä»£ç 
â”‚ 3. å·¥å‚çš„ä¼˜é›…ï¼šé€šè¿‡å·¥å‚æ¨¡å¼ï¼Œå¯¹è±¡çš„åˆ›å»ºå˜å¾—é€æ˜ä¸”å¯ç®¡ç†
â”‚
â”‚ ä¸‹ä¸€æ­¥ï¼š
â”‚
â”‚ â€¢ æˆ‘ä»¬å°†è¿›å…¥ç¬¬å››ç« ï¼šæ™ºèƒ½äº¤äº’ç•Œé¢å®ç°
â”‚ â€¢ è¿™æ˜¯æˆ‘ä»¬ä¸ç”¨æˆ·ç›´æ¥æ¥è§¦çš„åœ°æ–¹ï¼Œéœ€è¦æè‡´çš„ç”¨æˆ·ä½“éªŒ
â”‚ â€¢ Reactç»„ä»¶ + æ‹–æ‹½ç³»ç»Ÿ + æ ·å¼ç³»ç»Ÿï¼Œè®©æˆ‘ä»¬ä¸€èµ·æ‰“é€ æœ€é…·çš„AIäº¤äº’ç•Œé¢ï¼

--------

  # ğŸš€ YYCÂ³-AILP æ™ºèƒ½æµ®çª—ç³»ç»Ÿ - ç¼ºå¤±è¡¥å…¨å®Œæ•´æ–¹æ¡ˆï¼ˆç¬¬å››ç« ï¼‰

  > **ç»§ç»­æˆ‘ä»¬çš„æ—…ç¨‹ï¼** ğŸŒŸ
  > å‰ä¸‰ç« æˆ‘ä»¬æ„å»ºäº†åšå®çš„åŸºç¡€è®¾æ–½ã€æ ¸å¿ƒå¼•æ“å’Œæ™ºèƒ½çš„æ¨¡å‹é€‚é…ç³»ç»Ÿã€‚
  > ç°åœ¨æ˜¯æ—¶å€™ç»™ç³»ç»Ÿç©¿ä¸Š"å¤–è¡£"äº†â€”â€”æ™ºèƒ½äº¤äº’ç•Œé¢ã€‚
  > ä¸€ä¸ªå¥½çš„AIç³»ç»Ÿï¼Œä¸ä»…è¦æœ‰å¼ºå¤§çš„"å¤§è„‘"ï¼Œæ›´è¦æœ‰å‹å¥½çš„"é¢å­”"ã€‚
  > æœ¬ç« æˆ‘ä»¬å°†ç”¨Reactæ„å»ºä¸€ä¸ªå¯æ‹–æ‹½ã€å¯æ’æ‹”ã€æè‡´ä½“éªŒçš„æ™ºèƒ½æµ®çª—ï¼ğŸ’ª

  ---
